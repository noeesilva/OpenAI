m,Domain,Task,Organization(s),Organization Categorization,Author(s),Publication date,Year,Reference,Link,Citations,Inclusion criteria,Inclusion criteria met,Parameters,Training compute (FLOPs),Training dataset,Training dataset size (datapoints),Hidden layers,Inference compute (FLOPs),Training time (hours),Equivalent training time (hours),Inference time (ms),Training dataset size (GB),Approach,Dense or sparse model,Training objective,Training compute cost (2020 USD),Self-supervised training,Architecture,Compute Sponsor Categorization,Abstract
GPT-4,Multimodal,Language modelling,OpenAI,Industry,OpenAI,15/03/2023,2023,GPT-4 Technical Report,https://arxiv.org/abs/2303.08774,,SOTA Improvement,Yes,,2.80E+25,,,,,,,,,,,,,Yes,,,
Phenaki,Vision,Video generation,"Google Brain, University College London, University of Michigan",Industry - Academia Collaboration (Industry leaning),"Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan",05/10/2022,2022,Phenaki: Variable Length Video Generation From Open Domain Textual Description,https://arxiv.org/abs/2210.02399,,,No,1.80E+09,,,,,,,,,,,,,,Yes,,,
Minerva (540B),Language,Quantitative Reasoning Problems,Google Research,Industry,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",29/06/2022,2022,Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/abs/2206.14858,,SOTA Improvement,Yes,5.40E+11,2.74E+24,"PaLM, finetuned on Arxiv",6.14E+11,8.19E+11,,,,,,,,,3267257.75,Yes,,Industry,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them."
PaLM (540B),Language,Language modelling,Google Research,Industry,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",04/04/2022,2022,PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/abs/2204.02311,2.28E+02,SOTA Improvement,Yes,5.40E+11,2.53E+24,,5.85E+11,,,,,,,,,,3232806.53,Yes,,Industry,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
OPT-175B,Language,Language modelling,Meta AI,Industry,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",02/05/2022,2022,OPT: Open Pre-trained Transformer Language Models,https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/,,,No,1.75E+11,7.56E+23,,1.35E+11,,,,,,,,,,1654082.50,Yes,,Industry,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models"
Chinchilla,Language,Language modelling,DeepMind,Industry,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre",29/03/2022,2022,Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556,,SOTA Improvement,Yes,7.00E+10,5.76E+23,,1.05E+12,,,,,,,,,,753491.58,Yes,,Industry,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher."
Parti,Drawing,Text-to-image,Google Research,Industry,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",22/06/2022,2022,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/abs/2206.10789v1,,SOTA Improvement,Yes,2.00E+10,3.96E+23,"LAION-400M, FIT400M, JFT-4B ",4.80E+09,,,,,,,,,,486659.77,Yes,,Industry,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images."
LaMDA,Language,,Google,Industry,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",10/02/2022,2022,LaMDA: Language Models for Dialog Applications,https://arxiv.org/abs/2201.08239,,Historical significance,Yes,1.37E+11,3.55E+23,Infiniset,1.56E+09,,,,,,,,,,484957.20,Yes,,Industry,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency."
YaLM,Language,Language modelling,Yandex,Industry,,23/06/2022,2022,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,,No,1.00E+11,2.20E+23,,,,,,,,,,,,,,,Industry,
AlexaTM 20B,Language,Language modelling,Amazon,Industry,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",02/08/2022,2022,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/abs/2208.01448,,SOTA Improvement,Yes,1.98E+10,2.04E+23,mC4; Wikipedia,,,,2880,368640,,,,,,,,,Industry,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. "
BLOOM,Language,Language model,"Hugging Face, BigScience",Research Collective,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",11/8/2022,2022,,https://huggingface.co/bigscience/bloom,,,No,1.76E+11,1.80E+23,"""TB scale multilingual dataset""",,,,,,,,,,,,Yes,,,
GPT-NeoX-20B,Language,,EleutherAI,Research Collective,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",09/02/2022,2022,Announcing GPT- NeoX- 20B,https://blog.eleuther.ai/announcing-20b/,4.50E+01,Historical significance,Yes,2.00E+10,9.32E+22,The Pile,1.77E+11,,,,,,,,,,202407.46,Yes,,Industry,
Stable Diffusion (LDM-KL-8-G),Drawing,Text-to-image,"Stability AI, Runway",Industry,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer",13/04/2022,2022,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/abs/2112.10752,,Significant use,Yes,1.45E+09,5.00E+22,LAION-400M,4.00E+08,,,,,,,,,,,Yes,,,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL ."
Whisper,Speech,Audio Speech Recognition,OpenAI,Industry,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",21/09/2022,2022,Robust Speech Recognition via Large-Scale Weak Supervision,https://cdn.openai.com/papers/whisper.pdf,,SOTA Improvement,Yes,1.55E+09,4.65E+22,,5.58E+11,,,,,,,,,,,Yes,,,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing."
GLM-130B,Language,,Tsinghua KEG,Academia,,04/08/2022,2022,GLM-130B: An open bilingual pre-trained model,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,,,No,1.30E+11,4.60E+22,,,,,,,,,,,,,,,Industry,
AlphaCode,Language,Code generation,DeepMind,Industry,The Alpha Code team,02/02/2022,2022,Competition-Level Code Generation with AlphaCode,https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode,7.50E+01,SOTA improvement,Yes,,4.05E+22,,,,,,,,,,,,,Yes,,Industry,
NLLB,Language,Translation,Meta AI,Industry,"Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",06/07/2022,2022,No Language Left Behind: Scaling Human-Centered Machine Translation,https://research.facebook.com/publications/no-language-left-behind/?utm_source=twitter&utm_medium=organic_social&utm_campaign=nllb&utm_content=os-artifacts,1.90E+01,SOTA Improvement,Yes,5.45E+10,1.75E+22,,3.60E+11,,,,,,,,,,39175.64,Yes,,Industry,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb."
Primer,Language,,"Google Research, Brain Team",Industry,"DavidR.So, WojciechMan ́ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le",24/01/2022,2022,Primer: Searching for Efficient Transformers for Language Modeling,https://arxiv.org/abs/2109.08668,3.90E+01,,No,1.90E+09,7.10E+21,C4,1.73E+11,,,,,,,,,,9690.72,,,Industry,"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility."
Gato,Multimodal,,DeepMind,Industry,"Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",12/05/2022,2022,A Generalist Agent,https://www.deepmind.com/publications/a-generalist-agent,,Historical significance,Yes,1.18E+09,5.44E+21,,,,,,,,,,,,6781.08,,,Industry,"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato."
GPT-SW3,Language,Language modelling,"AI Sweden, RISE",Academia,"Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Ohman, Fredrik Carlsson, Magnus Sahlgren",25/06/2022,2022,,http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf,,,No,3.50E+09,1.30E+21,,,,,,,,,,,,,Yes,,,"Large-scale generative language models such as the GPT series (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020) have enjoyed considerable attention in recent years. This has been partly due to their unprecedented ability to generate coherent text, but also for their capacity for zero-shot performance - without any training examples, on a wide range of different tasks. A prerequisite for building such models is access to both large amounts of high-quality text data and powerful computational resources. This has proven to be a limiting factor for the development of large-scale models for languages other than English. With the goal of promoting the development of largescale generative models for other languages, we here present our work on developing and evaluating GPTSW3, a 3.5 billion parameter autoregressive language model, trained on a newly collected 100 GB Swedish corpus. To the best of our knowledge, this is the largest generative model for Swedish to date, and probably one of the bigger non-English models at the moment. In this paper, we collect the lessons learned by developing and evaluating this model, including challenges with data collection, training procedures, and validation activities."
Sparse all-MLP,Language,,MetaAI,Industry - Academia Collaboration (Industry leaning),"Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleifer, Hongyu Gong, Ves Stoyanov",14/04/2022,2022,Efficient Language Modeling with Sparse all-MLP,https://arxiv.org/abs/2203.06850,0.00E+00,SOTA Improvement,Yes,3.50E+09,2.17E+19,RoBERTa dataset,,,,,20.41,,,,,,80.29,Yes,,Industry,"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2× improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers."
LiMoE,Multimodal,Image classification,Google,Industry,"Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby",06/06/2022,2022,Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,https://arxiv.org/abs/2206.02770,15,,No,5.60E+09,2.00E+11,,,,,,,,,,,,,,,Industry,
data2vec (vision),Vision,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",20/01/2022,2022,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,Yes,7.05E+08,,ImageNet,1.28E+06,,,,,,,,,,,Yes,,Industry,
data2vec (speech),Speech,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",20/01/2022,2022,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,Yes,7.05E+08,,LS-960,7.88E+08,,,,,,,,,,,Yes,,Industry,
data2vec (language),Language,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",20/01/2022,2022,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,Yes,7.05E+08,,"Books Corpus, English Wikipedia",3.30E+09,,,,,,,,,,,Yes,,Industry,
InstructGPT,Language,,OpenAI,Industry,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",27/01/2022,2022,Training language models to follow instructions with human feedback,https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf,1.49E+02,Historical significance,Yes,,,,3.32E+04,,,,,,,,,,,Yes,,Industry,
RETRO-7B,Language,,DeepMind,Industry,"Sebastian Borgeaud†, Arthur Mensch†, Jordan Hoffmann†, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre",07/02/2022,2022,Improving language models by retrieving from trillions of tokens,https://arxiv.org/abs/2112.04426,5.90E+01,,No,7.50E+09,,,1.50E+12,,,,,,,,,,,Yes,Retrieval Transformer,Industry,
MuZero VP9,Other,Video compression,DeepMind,Industry,"Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann",14/02/2022,2022,MuZero with Self-competition for Rate Control in VP9 Video Compression,https://arxiv.org/abs/2202.06626,,,No,,,,,,,,,,,,,,,,,,
DeepNet,Language,Language modelling,Microsoft Research,Industry,"Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei",01/03/2022,2022,"DeepNet: Scaling Transformers to 1,000 Layers",https://arxiv.org/abs/2203.00555,,,No,3.20E+09,,,1.20E+10,,,,,,,,,,,,,Industry,
Statement Curriculum Learning,Language,Automated theorem proving,OpenAI,Industry,"Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ",02/03/2022,2022,Formal Mathematics Statement Curriculum Learning,https://arxiv.org/abs/2202.01344,,,No,,,,,,,,,,,,,,,,,Industry,
BaGuaLu,,,Tsinghua University,Academia,"Zixuan Ma, Jiaao He, Jiezhong Qiu, Huanqi Cao, Yunawei Wang",28/03/2022,2022,BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores,https://dl.acm.org/doi/abs/10.1145/3503221.3508417,1.20E+01,,No,1.45E+13,,,,,,,,,,,,,,Yes,,Academia,
DALL·E 2,Drawing,,OpenAI,Industry,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",06/04/2022,2022,Hierarchical Text-Conditional Image Generation with CLIP Latents,https://cdn.openai.com/papers/dall-e-2.pdf,,SOTA Improvement,Yes,3.50E+09,,"CLIP, DALL-E",6.50E+08,,,,,,,,,,,Yes,,Industry,
Flamingo,Multimodal,,DeepMind,Industry,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",29/04/2022,2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/abs/2204.14198,,SOTA Improvement,Yes,8.00E+10,,,,,,,,,,,,,,,,Industry,
Jurassic-X,Language,,AI21labs,Industry,"Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz",03/05/2022,2022,"MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system,,,No,7.00E+09,,,,,,,,,,,,,,,,Industry,
UL2,Language,,Google Research,Industry,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",10/05/2022,2022,Unifying Language Learning Paradigms,https://arxiv.org/abs/2205.05131v1,1.90E+01,SOTA Improvement,Yes,2.00E+10,,,,,,,,,,,,,,,,Industry,
Imagen,Vision,,Google Research,Industry,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li",23/05/2022,2022,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://imagen.research.google/,,,No,,,,,,,,,,,,,,,Yes,,Industry,
MetaLM,Multimodal,,Microsoft Research,Industry,"Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",13/06/2022,2022,Language Models are General-Purpose Interfaces,https://arxiv.org/abs/2206.06336v1,,SOTA Improvement,Yes,,,,,,,,,,,,,,,Yes,,Industry,
Make-A-Video,Text-to-Video,,Meta AI,Industry,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",29/09/2022,2022,Make-A-Video: Text-to-Video Generation without Text-Video Data,https://arxiv.org/abs/2209.14792,,SOTA Improvement,Yes,,,,,,,,,,,,,,,Yes,,,
Galactica,Language,Language modelling,Meta AI,Industry,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",15/11/2022,2022,Galactica: A Large Language Model for Science,https://galactica.org/static/paper.pdf,,,No,1.20E+11,,Galactica Corpus,,,,,,,,,,,,Yes,,,
CICERO,Games,Diplomacy,Meta AI,Industry,,22/11/2022,2022,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,https://www.science.org/doi/10.1126/science.ade9097,,,No,,,WebDiplomacy,,,,,,,,,,,,,,,
Imagen Video,Vision,Video generation,Google Brain,Industry,"Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans",05/10/2022,2022,Imagen Video: High Definition Video Generation with Diffusion Models,https://arxiv.org/abs/2210.02303,,,No,1.16E+10,,,,,,,,,,,,,,,,,
Megatron-LM (1T),Language,Text autocompletion,"Stanford, Microsoft Research, NVIDIA",Industry - Academia Collaboration (Industry leaning),"Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia",09/04/2021,2021,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/abs/2104.04473,1.08E+02,,No,1.00E+12,,,,,,,,,,,,,,Yes,,Industry,
Megatron-Turing NLG 530B,Language,,"Microsoft, NVIDIA",Industry,"Ali Alvi, Paresh Kharya",11/10/2021,2021,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model",https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,1.14E+02,,No,5.30E+11,1.35E+24,,2.03E+11,,,,,,,,,,3046994.09,Yes,,Industry,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations."
Gopher,Language,Language modelling,DeepMind,Industry,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving",08/12/2021,2021,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",https://deepmind.com/blog/article/language-modelling-at-scale,1.76E+02,,No,2.80E+11,6.31E+23,,2.25E+11,,,,,,,,,,891638.80,Yes,,Industry,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
Yuan 1.0,Language,,Inspur,Industry,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",12/10/2021,2021,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,https://arxiv.org/abs/2110.04725,1.20E+01,,No,2.45E+11,4.10E+23,,8.35E+11,,,,,,,,,,606364.75,Yes,,Industry,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones."
Jurassic-1-Jumbo,Language,,AI21 Labs,Industry,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",11/08/2021,2021,Jurassic-1: Technical Details and Evaluation,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,5.50E+01,,No,1.78E+11,3.70E+23,,2.25E+11,,,,,,,,,,805277.01,Yes,,Industry,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models."
Source 1.0,Language,Language modelling,Inspur,Industry,,11/10/2021,2021,,https://www.gwern.net/docs/ai/scaling/2021-10-11-xinzhiyuan-inspursource10gpt245b.html,,,No,2.46E+11,3.54E+23,,,,,,,,,,,,,,,,
ALIGN,Multimodal,Representation Learning,Google AI,Industry,"ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,YunhsuanSung, Zhen Li, and Tom Duerig",11/06/2021,2021,Scaling up visual and vision-language representation learning with noisy text supervision,https://arxiv.org/abs/2102.05918,6.41E+02,,No,8.20E+08,2.15E+23,,1.60E+09,,,,,,,,,,357760.33,Yes,,Industry,"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries."
Meta Pseudo Labels,Vision,Image Classification,"Google AI, Brain team",Industry,"Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",01/03/2021,2021,Meta pseudo labels,https://arxiv.org/abs/2003.10580,3.93E+02,SOTA Improvement,Yes,4.80E+08,2.12E+23,ImageNet,1.30E+08,,,,,,,,,,369462.82,Yes,,Industry,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL."
Switch,Language,Text autocompletion,Google Brain,Industry,"William Fedus, Barret Zoph, Noam Shazeer",11/01/2021,2021,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,5.11E+02,,No,1.60E+12,8.22E+22,,4.32E+11,,,,,,,,,,149825.60,Yes,Switch Transformer,Industry,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
"
GOAT,Games,Open ended play,DeepMind,Industry,Open- Ended Learning Team,27/07/2021,2021,Open-Ended Learning Leads to Generally Capable Agents,https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play,6.40E+01,,No,3.50E+06,7.80E+22,XLand,3.90E+11,,,,,,,,,,122418.97,Yes,,Industry,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning."
ProtT5-XXL,Other,Proteins,"Technical University of Munich, Med AI Technology, Google AI, NVIDIA, Oak Ridge National Laboratory",Industry - Academia Collaboration,"A Elnaggar, M Heinzinger, C Dallago, G Rihawi",04/05/2021,2021,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3,3.96E+02,,No,1.10E+10,7.37E+22,UniRef; BDF,3.93E+11,,,,,,,,,,123918.36,Yes,,Industry,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans."
HyperClova,Language,,Naver Corp,Industry,,25/05/2021,2021,Hyperclova,https://www.navercorp.com/promotion/pressReleasesView/30546,,,No,2.04E+11,6.30E+22,,5.60E+11,,,,,,,,,,103802.31,Yes,,Industry,
PanGu-α,Language,,PanGu-α team,Industry,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian",25/04/2021,2021,PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,https://arxiv.org/abs/2104.12369,6.50E+01,,No,2.07E+11,5.83E+22,Custom dataset,2.00E+11,,,,,,,,,,97802.06,Yes,unidirectional transformer decoder,Industry,"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-α, with up to 200 billion parameters. PanGu-α is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-α in performing various tasks under few-shot or zero-shot settings."
FLAN,Language,Language modelling,Google Research,Industry,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le",03/09/2021,2021,FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS,https://arxiv.org/pdf/2109.01652.pdf,2.40E+02,,No,1.37E+11,4.90E+22,,,,,,,,,,,,,,,Industry,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
DALL-E,Drawing,Text-to-image,OpenAI,Industry,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",05/01/2021,2021,Zero-Shot Text-to-Image Generation,https://openai.com/blog/dall-e/,9.82E+02,Significant use,Yes,1.20E+10,4.70E+22,,2.50E+08,,,,,,,,,,171537.13,Yes,,Industry,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion."
PLUG,Language,,Alibaba Group,Industry,,19/04/2021,2021,,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,,SOTA Improvement,Yes,2.70E+10,3.60E+22,,,,,,,,,,,,,,,,
CogView,Drawing,Text-to-image,"Tsinghua University, DAMO academy Alibaba",Industry - Academia Collaboration (Academia leaning),"M Ding, Z Yang, W Hong, W Zheng, C Zhou",26/05/2021,2021,CogView: Mastering Text-to-Image Generation via Transformers,https://arxiv.org/abs/2105.13290,1.41E+02,,No,4.00E+09,2.68E+22,,3.00E+07,,,,,,,,,,44452.39,Yes,,Industry,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E."
T0-XXL,Language,Language modelling,"Hugging Face, Brown University",Industry - Academia collaboration,"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush",15/10/2021,2021,,https://arxiv.org/abs/2110.08207,,,No,1.10E+10,1.79E+22,,,,,,,,,,,,,,,,"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL."
GPT-J-6B,Language,,,Research collective,Aran Komatsuzaki,01/05/2021,2021,GPT-J-6B: 6B JAX-Based Transformer,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,,,No,6.05E+09,1.50E+22,,1.60E+11,,,,,,,,,,25176.80,Yes,,Industry,
CLIP (ViT L/14@336px),Multimodal,Zero-shot image classification,Open AI,Industry,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",05/01/2021,2021,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,2.92E+03,,Yes,3.70E+08,1.05E+22,Custom image-text pairs from the internet,4.00E+08,,1.10E+08,,86016,,,,,,40146.99,Yes,,Industry,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL."
GPT-Neo,Language,,EleutherAI,Research collective,,21/03/2021,2021,GPT-Neo,https://www.eleuther.ai/projects/gpt-neo/,,,No,2.70E+09,7.90E+21,The Pile,8.86E+11,,,,,,,,,,13685.99,Yes,,Industry,
Wu Dao - Wen Lan,Multimodal,,BAAI,Non-profit,,01/03/2021,2021,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,No,1.00E+09,7.20E+21,,,,,,,,,,,,,Yes,,Industry,
DeBERTa,Language,,Microsoft,Industry,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",10/06/2021,2021,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/abs/2006.03654,6.61E+02,,No,1.50E+09,6.00E+21,,1.56E+10,,,,,,"160 (pretraining), ",,,,,Yes,,Industry,"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8)."
HuBERT,Language,,Facebook AI Research,Industry,"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",27/07/2021,2021,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,https://arxiv.org/abs/2106.07447,4.58E+02,SOTA Improvement,Yes,1.00E+09,5.54E+21,LibriSpeech,7.88E+08,,,,,,,,,,8632.11,Yes,,Industry,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets."
M6-10T,Multimodal,,Alibaba,Industry,"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang",08/10/2021,2021,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,https://arxiv.org/abs/2110.03888,1.70E+01,,No,1.00E+13,5.53E+21,BookCorpus; English Wikipedia,8.00E+09,,,,122880,,,,,,20073.49,Yes,,Industry,"Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called ""Pseudo-to-Real"" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI."
NÜWA,Multimodal,,"Microsoft Research, Peking University",Industry,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",24/11/2021,2021,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/abs/2111.12417,3.60E+01,SOTA Improvement,Yes,8.70E+08,4.84E+21,"Conceptual Captions, Moments in Time, VATEX",,,,,,,,,,,10446.84,Yes,,Industry,"This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL."
SEER,Vision,,"Facebook AI Research, Inria",Industry - Academia Collaboration (Industry leaning),"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",29/07/2021,2021,Self-supervised Pretraining of Visual Features in the Wild,https://arxiv.org/abs/2103.01988,1.38E+02,,No,1.30E+09,4.42E+21,,1.00E+09,,,,,,,,,,16058.80,Yes,,Industry,"Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL"
ViT-G/14,Vision,,"Google Research, Brain Team",Industry,"X Zhai, A Kolesnikov, N Houlsby, L Beyer",08/06/2021,2021,Scaling Vision Transformers,https://arxiv.org/abs/2106.04560,3.02E+02,,No,1.80E+09,3.40E+21,JFT-3B,3.00E+09,,,,,,,,,,5541.84,Yes?,,Industry,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class."
NEO (DL:RM-2022),Recommendation,,Facebook,Industry,"D Mudigere, Y Hao, J Huang, A Tulloch",15/09/2021,2021,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,https://arxiv.org/abs/2104.05158,1.00E+01,,No,3.00E+12,1.10E+21,,,,,,,,,,,,2394.07,,,Industry,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments."
Wu Dao - Wen Yuan,Language,,BAAI,Non-profit,,01/03/2021,2021,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,No,2.60E+09,6.50E+20,,,,,,,,,,,,,Yes,,Industry,
PAGnol-XL,Language,Language modelling,"LightOn, LPENS",Industry - Academia Collaboration,"Julien Launay, E.L. Tommasone, Baptiste Pannier, François Boniface, Amélie Chatelain, Alessandro Cappelli, Iacopo Poli, Djamé Seddah",16/10/2021,2021,,https://arxiv.org/pdf/2110.08554.pdf,,,No,1.50E+09,2.59E+20,,,,,,,,,,,,,Yes,,,"Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained to date for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pretraining and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available."
Wu Dao - Wen Hui,Multimodal,,BAAI,Non-profit,,01/03/2021,2021,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,No,1.13E+10,1.16E+20,,,,,,,,,,,,,Yes,,Industry,
Transformer local-attention (NesT-B),Vision,,"Google cloud AI, Google research, Rutgers University",Industry - Academia Collaboration (Industry Leaning),"Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Tomas Pfister",26/05/2021,2021,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://arxiv.org/abs/2105.12723v4,5.73E+03,Highly cited,Yes,,2.41E+19,Imagenet-1k,1.28E+06,,,,,,,,,,39.51,Yes,,Industry,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8× faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL."
ERNIE 3.0,Language,,Baidu Inc. ,Industry,"Y Sun, S Wang, S Feng, S Ding, C Pang",05/07/2021,2021,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,http://research.baidu.com/Blog/index-view?id=160,1.00E+02,,No,1.00E+10,2.35E+18,,6.68E+11,,,,,,,,,,3.83,Yes,Transformer-XL: Transformer with auxilary recurrence memory module,Industry,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Drawing,,UC Berkeley,Academia,"Jonathan Ho, Ajay Jain, Pieter Abbeel",11/06/2021,2021,Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239,9.18E+02,SOTA Improvement,Yes,2.56E+08,1.60E+18,,3.03E+06,,,,,,,,,,2.60,,,Academia,
PCL-BAIDU Wenxin (ERNIE 3.0 Titan),Language,,Baidu,Industry,"Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",23/12/2021,2021,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/abs/2112.12731,1.10E+01,,No,2.60E+11,3.14E+11, ERNIE 3.0 Corpus,6.68E+11,,,,,,,,,,,,,Industry,
CLIP (ResNet-50),Multimodal,Zero-shot image classification,Open AI,Industry,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",05/01/2021,2021,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,2.92E+03,,Yes,8.86E+07,,Custom image-text pairs from the internet,4.00E+08,,7.00E+06,,,,,,,,,,,Industry,
BigSSL,Speech,Audio speech recognition,"Google, Apple",Industry,"Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu",10/01/2021,2021,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,https://arxiv.org/abs/2109.13226,4.10E+01,,No,8.00E+09,,,2.56E+12,,,,,,,,,,,,,Industry,
Rational DQN Average,Games,Atari Games,TU Darmstadt,Academia,"Q Delfosse, P Schramowski, A Molina",18/02/2021,2021,Recurrent Rational Networks,https://openreview.net/forum?id=gnRmI8TatHV,3.00E+00,SOTA improvement,Yes,1.68E+06,,,,,,,,,,,,,,,,Academia,
M6-10B,Multimodal,,"Tsinghua University, Alibaba Group",Industry - Academia Collaboration (Industry leaning),"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",01/03/2021,2021,M6: A Chinese Multimodal Pretrainer,https://arxiv.org/abs/2103.00823,7.60E+01,,No,1.00E+10,,,1.90E+12,,,,,,,,,,,,,Industry,
M6-100B,Multimodal,,"Tsinghua University, Alibaba Group",Industry - Academia Collaboration (Industry leaning),"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",01/03/2021,2021,M6: A Chinese Multimodal Pretrainer,https://arxiv.org/abs/2103.00823,7.60E+01,,No,1.00E+11,,,1.90E+12,,,,,,,,,,,,,Industry,
Wu Dao - Wen Su,Other,Proteins,BAAI,Non-profit,,01/03/2021,2021,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,No,,,,,,,,,,,,,,,Yes,,Industry,
Generative BST,Language,,Facebook AI Research,Industry,"Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",05/03/2021,2021,Recipes for building an open-domain chatbot,https://arxiv.org/abs/2004.13637,5.03E+02,SOTA Improvement,Yes,9.40E+09,,,,,,,,,,,,,,,,Industry,
M6-T,Multimodal,,Alibaba Group,Industry,"An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",05/03/2021,2021,M6-T: Exploring Sparse Expert Models and Beyond,https://arxiv.org/abs/2105.15082,7.60E+01,SOTA Improvement,Yes,1.00E+12,,M6-Corpus,1.90E+12,,,,,,,,,,,Yes,,Industry,
DLRM-12T,Recommendation,,Facebook AI,Industry,"Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",12/04/2021,2021,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,https://arxiv.org/abs/2104.05158,1.00E+01,,No,1.20E+13,,,,,,,,,,,,,,,,Industry,
Wu Dao 2.0,Multimodal,,BAAI,Non-profit,A Tarantola,01/06/2021,2021,China's gigantic multi-modal AI is no one-trick pony,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,,,No,1.75E+12,,,,,,,,,,,,,,,,Industry,
Codex,Language,Code autocompletion,Open AI,Industry,"Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",07/07/2021,2021,Evaluating Large Language Models Trained on Code,https://openai.com/blog/openai-codex/,3.01E+02,Significant use,Yes,1.20E+10,,,3.18E+10,,,,,,,,,,,Yes,,Industry,
XLMR-XXL,Language,,Facebook AI Research,Industry,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",17/08/2021,2021,Larger-Scale Transformers for Multilingual Masked Language Modeling,https://arxiv.org/abs/2105.00572,1.80E+01,SOTA Improvement,Yes,1.07E+10,,CC100,1.25E+11,,,,,,,,,,,,,Industry,
FLAN,Language,,Google Research,Industry,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le",03/09/2021,2021,Finetuned Language Models Are Zero-Shot Learners,https://arxiv.org/abs/2109.01652,2.40E+02,SOTA Improvement,Yes,1.37E+11,,,1.87E+12,,,,,,,,,,,Yes,,Industry,
MEB,Search,,Microsoft Bing,Industry,"W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi",04/09/2021,2021,Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance,https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/,2.60E+01,Significant use,Yes,1.35E+11,,,,,,,,,,,,,,,,Industry,
T0-XXL,Language,,"Hugging Face, Brown University, BigScience",Industry - Academia Collaboration (Industry leaning),"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, Alexander M. Rush",15/10/2021,2021,Multitask Prompted Training Enables Zero-Shot Task Generalization,https://arxiv.org/abs/2110.08207,2.43E+02,,No,1.10E+10,,,,,,,,,,,,,,,,Industry,
Cloob,Multimodal,,"Ellis Unit Linz and LIT AI Lab, Johannes Kepler University, IARIA Vienna, HERE Technologies",Industry - Academia Collaboration (Academia Leaning),"Andreas Fürst ∗Elisabeth Rumetshofer ∗Johannes Lehner,Viet Tran,Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter",21/10/2021,2021,CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,https://arxiv.org/abs/2110.11316,2.00E+01,,No,,,,1.50E+07,,,,,,,,,,,,,Industry,
EfficientZero,Games,,"Tsinghua University, UC Berkeley, Shanghai Qi Zhi institute",Academia,"Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",30/10/2021,2021,Mastering Atari Games with Limited Data,https://arxiv.org/abs/2111.00210,4.00E+01,,No,,,,,,,,,,,,,,,,,Academia,
Japanese dialog transformers,Language,,NTT Communication Science Laboratories,Industry,"Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro",09/11/2021,2021,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,https://arxiv.org/abs/2109.05217,3.10E+01,,No,1.60E+09,,,2.10E+09,,,,,,,,,,,,,Industry,
Player of Games,Games,,DeepMind,Industry,"Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",06/12/2021,2021,Player of Games,https://arxiv.org/abs/2112.03178,9.00E+00,SOTA Improvement,Yes,,,,,,,,,,,,,,,,,Industry,
GLIDE,Drawing,,OpenAI,Industry,"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam Pamela Mishkin Bob McGrew IlyaSutskever MarkChen",20/12/2021,2021,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/abs/2112.10741,2.38E+02,,No,3.50E+09,,,2.50E+08,,,,,,,,,,,,,Industry,
XGLM,Language,,Meta AI,Industry,"Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",20/12/2021,2021,Few-shot Learning with Multilingual Language Models,https://arxiv.org/pdf/2112.10668.pdf,1.20E+01,SOTA improvement,Yes,7.50E+09,,Subset of CC100-XL,1.74E+09,,,,,,,,,,,Yes,,Industry,
ERNIE-ViLG,Multimodal,Vision-language generation,Baidu,Industry,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",31/12/2021,2021,ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,https://arxiv.org/abs/2112.15283,1.20E+01,,No,1.00E+10,,,1.45E+08,,,,,,,,,,,Yes,,,
GPT-3 175B (davinci),Language,Text autocompletion,OpenAI,Industry,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",28/05/2020,2020,Language models are Few- Shot Learners,https://arxiv.org/abs/2005.14165,1.53E+03,Highly cited,Yes,1.75E+11,3.14E+23,CommonCrawl; WebText2; Books1; Books2; Wikipedia,3.74E+11,,7.40E+14,,,,45TB,,,,1131415.12,Yes,,Industry,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
Meena,Language,Text autocompletion,Google AI,Industry,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",28/01/2020,2020,Towards a Human-like Open-Domain Chatbot,https://arxiv.org/abs/2001.09977,6.15E+02,,No,2.60E+09,1.12E+23,,4.00E+10,,,,,,,,,,263099.94,Yes,Evolved Transformer seq2seq model,Industry,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated."
iGPT-XL,Drawing,Image completion,Open AI,Industry,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",17/06/2020,2020,Generative Pretraining from Pixels,https://openai.com/blog/image-gpt/,6.89E+02,,No,6.80E+09,3.30E+22,ILSVRC 2012,9.60E+06,,,,,,,,,,120440.96,Yes,,Industry,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features."
GShard (dense),Language,Translation,Google Brain,Industry,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",30/06/2020,2020,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/abs/2006.16668,2.95E+02,,No,2.30E+09,2.60E+22,,2.60E+11,,,,,,,,,,55219.61,Yes,,Industry,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
Turing NLG,Language,Text autocompletion,Microsoft,Industry,C Rosset,13/02/2020,2020,Turing-NLG: A 17-billion-parameter language model by Microsoft,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,1.14E+02,,No,1.70E+10,1.57E+22,,3.48E+10,,3.60E+13,,,,,,,Next token prediction,58395.62,Yes,,Industry,
GShard (600B),Language,Translation,Google Brain,Industry,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",30/06/2020,2020,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/abs/2006.16668,2.95E+02,,No,6.00E+11,1.33E+22,,2.60E+11,,,,,,,,,,27609.81,Yes,,Industry,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
ViT-H/14,Vision,Image representation,"Google Research, Brain Team",Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",28/09/2020,2020,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://openreview.net/forum?id=YicbFdNTTy,1.91E+03,Highly cited,Yes,,1.28E+22,Imagenet-1k,1.28E+06,,,,,,,,,,25757.45,,,Industry,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
iGPT-L,Drawing,Image completion,Open AI,Industry,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",17/06/2020,2020,Generative Pretraining from Pixels,https://openai.com/blog/image-gpt/,6.89E+02,,No,1.36E+09,8.91E+21,ILSVRC 2012,9.60E+06,,,,,,,,,,32482.56,Yes,,Industry,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features."
ALBERT-xxlarge,Language,,"Google research, Toyota Technological Institute at Chicago",Industry - Academia Collaboration,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",09/02/2020,2020,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.,https://arxiv.org/abs/1909.11942,2.18E+03,Highly cited,Yes,2.35E+08,2.54E+21,,3.30E+09,,2.50E+12,,17408,,,,,,5924.43,Yes,,Industry,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL."
AraGPT2-Mega,Language,,American University of Beirut,Academia,"W Antoun, F Baly, H Hajj",31/12/2020,2020,AraGPT2: Pre-Trained Transformer for Arabic Language Generation,https://arxiv.org/abs/2012.15520,2.40E+01,,No,1.50E+09,2.00E+21,,8.80E+09,,,,,,,,,,3685.43,Yes,,Academia,"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP."
CPM-Large,Language,,"Tsinghua University, BAAI",Academia,"Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su",01/12/2020,2020,CPM: A Large-scale Generative Chinese Pre-trained Language Model,https://arxiv.org/abs/2012.00413,4.90E+01,,No,2.60E+09,1.80E+21,,1.67E+10,,,,,,,,,,6569.51,Yes,Left-To-Right Transformer Decoder,Academia,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL."
Once for All,Vision,,MIT-IBM Watson AI Lab,Industry,"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",29/04/2020,2020,Once for all: Train one network and specialize it for efficient deployment.,https://arxiv.org/abs/1908.09791,7.33E+02,,No,7.70E+06,1.78E+21,Imagenet,,,,,,,,,,,6569.51,,,Industry,"We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL."
wave2vec 2.0 LARGE,Speech,Speech completion,Facebook,Industry,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",22/10/2020,2020,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,https://arxiv.org/abs/2006.11477,4.10E+02,SOTA Improvement,Yes,3.17E+08,4.34E+20,LibriSpeech,4.37E+10,,,,,,,,,,1569.38,Yes,,Industry,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."
DLRM-2021,Recommendation,,Facebook AI ,Industry,"D Mudigere, Y Hao, J Huang, A Tulloch",01/07/2020,2020,"High- performance, Distributed Training of Large scale Deep Learning Recommendation Models",https://www.arxiv-vanity.com/papers/2104.05158/,1.00E+01,,No,1.00E+12,3.00E+20,,,,,,,,,,,,1094.92,,,Industry,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebookand are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40 × speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments."
ProGen,Other,Protein generation,"Salesforce research, Stanford",Industry - Academia Collaboration,"A Madani, B McCann, N Naik, NS Keskar",13/03/2020,2020,ProGen: Language Modeling for Protein Generation,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,1.31E+02,,No,1.20E+09,2.70E+20,,,,,,,,,,,,623.75,Yes,,Industry,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy."
ELECTRA,Language,,"Stanford University, Google Brain",,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",23/03/2020,2020,ELECTRA: pre-training text encoders as discriminators rather than generators,https://arxiv.org/abs/2003.10555v1,,,No,,2.00E+20,,,,,,,,,,,,,Yes,,,"Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute."
KEPLER,Language,Relation Extraction,"Tsinghua University, Princeton, Mila- Quebec AI, University de Montreal, HEC, CIFAR",Academia,"Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",23/11/2020,2020,KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.,https://arxiv.org/abs/1911.06136,2.56E+02,,No,1.10E+08,1.24E+20,Wikipedia+BookCorpus,3.30E+09,,,,,,,,,,437.97,Yes,,Academia,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL."
AlphaFold,Other,Protein folding prediction,DeepMind,Industry,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu & Demis Hassabis",15/01/2020,2020,Improved protein structure prediction using potentials from deep learning,https://www.nature.com/articles/s41586-019-1923-7,8.40E+02,SOTA improvement,Yes,6.90E+07,1.00E+20,,,,,,,,,,,Score,241.59,Yes,,Industry,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7."
Theseus 6/768,Language,Text autocompletion,"UC San Diego, Beihang University, Microsoft",Industry - Academia Collaboration,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou",07/02/2020,2020,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,https://arxiv.org/abs/2002.02925,1.23E+02,,No,6.60E+07,,,,,1.13E+10,,,,,,,,,,,Industry,
Perceiver IO,Multimodal,,DeepMind,Industry,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,
Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira",08/02/2020,2020,Perceiver IO: A General Architecture for Structured Inputs & Outputs,https://arxiv.org/abs/2107.14795,1.43E+02,,No,,,,,,,,,,,,,,,,,Industry,
SimCLR,Drawing,Image completion,"Google Research, Brain Team",Industry,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",13/02/2020,2020,A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,2.16E+03,Highly cited,Yes,3.75E+08,,,,,,,,,,,,,,,,Industry,
ELECTRA,Language,Text autocompletion,"Stanford, Google Brain",Industry - Academia Collaboration,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",23/03/2020,2020,Electra: pre-training text encoders as discriminators rather than generators,https://arxiv.org/abs/2003.10555v1,8.42E+02,,No,3.35E+08,,,,,7.90E+10,,,,,,,,,,,Industry,
MetNet,Other,Weather prediction,Google,Industry,"Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",24/03/2020,2020,MetNet: A Neural Weather Model for Precipitation Forecasting,https://arxiv.org/abs/2003.12140,1.35E+02,,No,,,,,,,,,,,,,,,,,Industry,
Agent57,Games,Atari,DeepMind,Industry,"AP Badia, B Piot, S Kapturowski",30/03/2020,2020,Agent57: Outperforming the Atari Human Benchmark,https://arxiv.org/abs/2003.13350,3.45E+02,,No,,,,,,,,,,,,,,,,,Industry,
,Vision,Person re-identification,"Xiamen University, Australian National University, Carnegie Mellon University",Academia,"Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang ",03/04/2020,2020, Random Erasing Data Augmentation ,https://arxiv.org/abs/1708.04896,1.42E+03,,Yes,,,,,,,,,,,,,,,,,Academia,
MobileBERT,Language,Text autocompletion,"Carnegie Mellon University, Google Brain",Industry - Academia Collaboration (Industry leaning),"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou",06/04/2020,2020,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,https://arxiv.org/abs/2004.02984,3.92E+02,,No,2.53E+07,,,,,5.36E+09,,,,,,,,,,,Industry,
CURL,Games,Atari Games,UC Berkeley,Academia,"A Srinivas, M Laskin, P Abbeel",08/04/2020,2020,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,https://arxiv.org/abs/2004.04136v4,2.90E+02,SOTA improvement,Yes,9.07E+05,,,,,,,,,,,,,,,,Academia,
Go-explore,Games,Atari,"Uber AI, OpenAI",Industry,"A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune",27/04/2020,2020,"First return, then explore",https://arxiv.org/abs/2004.12919,1.79E+02,,No,,,,,,,,,,,,,,,,,Industry,
SqueezeBERT,Language,Text autocompletion,Berkeley,Academia,"Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer",10/06/2020,2020,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,https://arxiv.org/abs/2006.11316,6.20E+01,,No,5.11E+07,,,,,7.42E+09,,,,,,,,,,,Academia,
Hopfield Networks (2020),Other,,"Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo",Academia,"Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter",16/07/2020,2020,Hopfield Networks is All You Need,https://arxiv.org/abs/2008.02217,1.82E+02,,No,,,,,,,,,,,,,,,,,Academia,
EfficientDet,Vision,Object detection,"Google Research, Brain Team",Industry,"Mingxing Tan, Ruoming Pang, Quoc V. Le",27/07/2020,2020,EfficientDet: Scalable and Efficient Object Detection,https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html,7.06E+02,,No,5.20E+07,,,,,3.25E+11,,,,,,,,,,,Industry,
ERNIE-GEN (large),Language,Language Generation,Baidu,Industry,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",06/08/2020,2020,ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://arxiv.org/abs/2001.11314,9.20E+01,,No,3.40E+08,,,,,,,,,16GB,,,,,,,Industry,
ViT-Base/32,Vision,Image representation,"Google Research, Brain Team",Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",22/10/2020,2020,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,7.29E+02,,No,8.60E+07,,,,,,,,"0.25 (512px image, one TPUv3 core)",,,,Score,,,,Industry,
ViT-Huge/14,Vision,Image representation,"Google Research, Brain Team",Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",22/10/2020,2020,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,7.29E+02,,No,6.32E+08,,,,,,,,"100 (512px image, one TPUv3 core)",,,,,,,,Industry,
SimCLRv2,,,"Google Research, Brain team",Industry,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton",26/10/2020,2020,Big self- supervised models are strong semi-supervised learners.,https://arxiv.org/abs/2006.10029,4.57E+02,,No,7.95E+08,,,1.28E+06,,,,,,,,,,,,,Industry,
AlphaFold2,Other,Protein folding prediction,DeepMind,Industry,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",30/11/2020,2020,High Accuracy Protein Structure Prediction Using Deep Learning,https://www.nature.com/articles/s41586-021-03819-2,2.40E+01,"Important context, significant use",Yes,,,,,,,,,,,,,,,,,Industry,
VQGAN + CLIP,Drawing,Text-to-image,Heidelberg University,Academia,"Patrick Esser, Robin Rombach, Björn Ommer",17/12/2020,2020,Taming Transformers for High-Resolution Image Synthesis,https://github.com/CompVis/taming-transformers,4.94E+02,,No,,,,,,,,,,,,,,,,,Academia,
AlphaStar,Games,StarCraft,DeepMind,Industry,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",30/10/2019,2019,Grandmaster level in StarCraft II using multi-agent reinforcement learning,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,1.04E+03,Highly cited,Yes,1.39E+08,2.02E+23,,,,,,,,,,,,512765.27,,,Industry,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players."
OpenAI Five,Games,Dota 2,OpenAI,Industry,"J Raiman, S Zhang, F Wolski",13/12/2019,2019,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/abs/1912.06680,4.54E+02,SOTA improvement,Yes,1.59E+08,6.70E+22,,4.54E+11,,,,,,,,,,166042.11,Yes,,Industry,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
"
Megatron-BERT,Language,,NVIDIA,Industry,"M Shoeybi, M Patwary, R Puri, P LeGresley",17/09/2019,2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,5.57E+02,,No,3.90E+09,5.68E+22,,3.48E+10,,,,,,,,,,208034.39,Yes,,Industry,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%)."
T5-11B,Language,Text autocompletion,Google,Industry,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",23/10/2019,2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,1.54E+03,Highly cited,Yes,1.10E+10,4.05E+22,Colossal Clean Crawled Corpus (C4),1.50E+11,,,,,,,,,,105686.20,Yes,Transformer (encoder-decoder performed best),Industry,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."
OpenAI Five Rerun,Games,Dota 2,OpenAI,Industry,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
Przemysław “Psyho"" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",13/12/2019,2019,Dota 2 with Large Scale Deep Reinforcement Learning,https://cdn.openai.com/dota-2.pdf,1.00E+03,SOTA improvement,Yes,1.59E+08,1.30E+22,,5.31E+10,,,,,,,,,,32217.13,Yes,,Industry,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task."
T5-3B,Language,Text autocompletion,Google,Industry,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",23/10/2019,2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,1.54E+03,Highly cited,Yes,3.00E+09,1.04E+22,Colossal Clean Crawled Corpus (C4),1.50E+11,,,,,,,,,,25777.12,Yes,Transformer (encoder-decoder performed best),Industry,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."
"Megatron-LM (Original, 8.3B)",Language,,NVIDIA,Industry,"M Shoeybi, M Patwary, R Puri, P LeGresley",17/09/2019,2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,5.57E+02,,No,8.30E+09,9.10E+21,,3.48E+10,,1.80E+13,,,,,,,,33212.51,Yes,,Industry,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%)."
FTW,Games,Capture the flag,DeepMind,Industry,"M Jaderberg, WM Czarnecki, I Dunning, L Marris",31/05/2019,2019,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://deepmind.com/research/publications/capture-the-flag,4.25E+02,,No,1.26E+08,7.26E+21,,,,1.21E+12,,,,,,,,21045.02,Yes,,Industry,
MnasNet-A1 + SSDLite,Vision,Performing image classification and object detection on mobile devices,Google ,Industry,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",29/05/2019,2019,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,1.43E+03,Highly cited,Yes,4.90E+06,1.50E+21,MS COCO,1.18E+05,,,,,,,,,,4331.00,,,Industry,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL"
MnasNet-A3,Vision,Performing image classification and object detection on mobile devices,Google ,Industry,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",29/05/2019,2019,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,1.43E+03,Highly cited,Yes,5.20E+06,1.50E+21,ImageNet,1.28E+06,,,,,,,,,,4331.00,,,Industry,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL"
GPT-2,Language,,OpenAI,Industry,"A Radford, J Wu, R Child, D Luan, D Amodei",14/02/2019,2019,Language Models are Unsupervised Multitask Learners,https://openai.com/blog/better-language-models/,1.70E+03,Highly cited,Yes,1.50E+09,1.49E+21,,3.00E+09,,3.40E+12,,,,40,,,,4692.89,Yes,,Industry,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
Rubik's cube,Robotics,,Open AI,Industry,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang
",15/10/2019,2019,Solving Rubik’s Cube with a Robot Hand,https://arxiv.org/abs/1910.07113,5.18E+02,,No,2.78E+07,8.54E+20,,6.24E+07,,,,,,,,,,3102.27,,,Industry,"We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL"
MuZero,Games,Atari Games,DeepMind,Industry,"J Schrittwieser, I Antonoglou, T Hubert, K Simonyan",19/11/2019,2019,Mastering Atari Go Chess and Shogi by Planning with a Learned Model,https://arxiv.org/abs/1911.08265v2,4.12E+02,SOTA improvement,Yes,3.69E+07,4.80E+19,,2.00E+10,,,,,,,,,,121.18,Yes,,Industry,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules."
ProxylessNAS,Vision,,MIT,Academia,"Han Cai, Ligeng Zhu, and Song Han",23/02/2019,2019,ProxylessNAS: Direct neural architecture search on target task and hardware,https://arxiv.org/abs/1812.00332,9.96E+02,,No,,3.71E+19,ImageNet,1.28E+06,,2.63E+11,,200,5.1,,,,,135.04,,,Academia,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."
ObjectNet,Vision,Object recognition,MIT,Academia,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz",06/09/2019,2019,Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,2.39E+03,Highly cited,Yes,3.80E+07,1.94E+19,Internal data,5.00E+04,,,,108,,,,,,50.79,,,Academia,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance."
AlphaX-1,Vision,Neural architecture search for computer vision,Brown and Facebook AI Research,Industry - Academia Collaboration (Academia leaning),"Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",02/10/2019,2019,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,7.20E+01,,No,5.79E+08,7.60E+18,ImageNet,,,,,,,,,,,24.10,,,Industry,
DLRM-2020,Recommendation,,Facebook AI,Industry,"M Naumov, D Mudigere, HJM Shi, J Huang",31/05/2019,2019,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://arxiv.org/abs/1906.00091,3.45E+02,,No,1.00E+11,4.00E+18,,,,,,,,,,,,14.60,,,Industry,
Cross-lingual alignment,,,"Tel Aviv University, MIT",Academia,"Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",04/04/2019,2019,"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",https://arxiv.org/abs/1902.09492,1.67E+02,,No,,2.56E+18,,,,3.66E+12,,,,,,,,7.83,,,Academia,
Decoupled weight decay regularization,Vision,Image classification,University of Freiburg,Academia,Ilya Loshchilov and Frank Hutter,04/01/2019,2019,Decoupled weight decay regularization.,https://arxiv.org/abs/1711.05101,2.06E+03,Highly cited,Yes,3.65E+07,2.47E+18,CIFAR-10,5.00E+04,,1.73E+09,,,,,,,,8.07,,,Academia,
Hide and Seek,Games,Hide and Seek,OpenAI,Industry,"B Baker, I Kanitscheider, T Markov, Y Wu",17/09/2019,2019,Emergent Tool Use From Multi-Agent Autocurricula,https://openai.com/blog/emergent-tool-use/,5.00E+02,,No,1.60E+06,3.04E+17,,3.17E+10,,,,,,,,,,0.80,,,Industry,
Hanabi 4 player,Games,Hanabi,"DeepMind, University of Oxford, Google Brain, Carnegie Mellon University, ",Industry - Academia Collaboration (Industry leaning),,01/02/2019,2019,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/abs/1902.00506,2.29E+02,,No,7.64E+05,9.16E+16,,,,,,,,,,,,0.34,,,Industry,
Pluribus,Games,Poker,Facebook AI Research,Industry - Academia collaboration,"Noam Brown, Tuomas Sandholm",11/07/2019,2019,Superhuman AI for multiplayer poker,https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf,5.75E+02,,No,,6.60E+16,,,,,,,,,,,,,,,,
Transformer ELMo,Language,"Univeristy of Washington, Allen Instititute for Artificial Intelligence",AI2,Industry - Academia Collaboration (Industry leaning),"ME Peters, M Neumann, L Zettlemoyer",01/01/2019,2019,Dissecting Contextual Word Embeddings: Architecture and Representation,https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59,3.08E+02,,No,4.65E+08,,,,,,,,,,,,,,,,Industry,
MT-DNN,Language,,Microsoft,Industry,"X Liu, P He, W Chen, J Gao",31/01/2019,2019,Multi-Task Deep Neural Networks for Natural Language Understanding,https://arxiv.org/abs/1901.11504,5.53E+02,,No,3.30E+08,,,,,,,,,,,,Encoder-decoder,,,Multilayer Bidirectional Transformer Encoder,Industry,
,Language,Speech recognition,Google Brain,Industry," Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le",18/04/2019,2019,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://arxiv.org/abs/1904.08779,1.41E+03,Highly cited,Yes,,,,,,,,,,,,,Encoder-decoder,,,,Industry,
,Vision,Semantic segmentation,Chinese Academy of Sciences,Industry - Academia Collaboration (Academia leaning),"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu",21/04/2019,2019,Dual Attention Network for Scene Segmentation,https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html,1.99E+03,,Yes,,,,,,,,,,,,,,,,,Industry,
ResNet-50 Billion-scale,Vision,Image classification,OpenAI,Industry,"A Radford, J Wu, R Child, D Luan, D Amodei",02/05/2019,2019,Language Models are Unsupervised Multitask Learners,https://paperswithcode.com/paper/language-models-are-unsupervised-multitask,1.70E+03,Highly cited,Yes,2.60E+07,,,,,,,,,,,,,,,,Industry,
ResNeXt-101 Billion-scale,Vision,Image classification,Facebook AI,Industry,"IZ Yalniz, H Jégou, K Chen, M Paluri",02/05/2019,2019,Billion-scale semi-supervised learning for image classification,https://arxiv.org/abs/1905.00546,3.19E+02,,No,1.93E+08,,,,,,,,,,,,,,,,Industry,
CPC v2,Drawing,Image completion,"DeepMind, Berkeley",Industry - Academia Collaboration (Industry leaning),,22/05/2019,2019,Data-Efficient Image Recognition with Contrastive Predictive Coding,https://arxiv.org/abs/1905.09272,4.91E+02,,No,3.03E+08,,,,,,,,,,,,,,,,Industry,
EfficientNet-L2,Vision,Image classification,Google,Industry,"M Tan, Q Le",28/05/2019,2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/abs/1905.11946,3.19E+03,Highly cited,Yes,4.80E+08,,,,,3.90E+08,,,,,,,Score,,,,Industry,
Grover-Mega,Language,,University of Washington,Industry - Academia Collaboration (Academia leaning),"R Zellers, A Holtzman, H Rashkin, Y Bisk",29/05/2019,2019,Defending Against Neural Fake News,https://arxiv.org/abs/1905.12616,5.43E+02,,No,1.50E+09,,,,,,,,,,,,,,,,Industry,
XLM,Language,,Facebook,Industry,"G Lample, A Conneau",01/06/2019,2019,Cross-lingual Language Model Pretraining,https://arxiv.org/abs/1901.07291,6.78E+02,,No,6.65E+08,,,,,,,,,,,,,,,,Industry,
XLNet,Language,,"Carnegie Mellon University, Google AI Brain ",Industry - Academia Collaboration,"Z Yang, Z Dai, Y Yang, J Carbonell",01/06/2019,2019,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/abs/1906.08237,3.06E+03,Highly cited,Yes,3.40E+08,,,,,,,,,,,,,,,,Industry,
AMDIM,Drawing,Image completion,Microsoft Research,Industry,"Philip Bachman, R Devon Hjelm, William Buchwalter",03/06/2019,2019,Learning Representations by Maximizing Mutual Information Across Views,https://arxiv.org/abs/1906.00910,4.86E+02,,No,6.26E+08,,,,,,,,,,,,,,,,Industry,
FixRes ResNeXt-101 WSL,Vision,Image classification,Facebook AI,Industry,"H Touvron, A Vedaldi, M Douze, H Jégou",14/06/2019,2019,Fixing the train-test resolution discrepancy,https://arxiv.org/abs/1906.06423,4.05E+02,,No,8.29E+08,,,9.40E+08,,,,,,,,,,,,,Industry,
RoBERTa,Language,,Facebook,Industry,"Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen",01/07/2019,2019,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/abs/1907.11692,1.51E+03,Highly cited,Yes,3.55E+08,,,,,,,,,,,,,,,,Industry,
BigBiGAN,Drawing,Image completion,Google,Industry,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",04/07/2019,2019,Large Scale Adversarial Representation Learning,https://arxiv.org/abs/1907.02544,4.03E+02,,No,8.60E+07,,,,,,,,,,,,,,,,Industry,
ALBERT,Language,,"Google, TTIC",Industry - Academia Collaboration,"Z Lan, M Chen, S Goodman, K Gimpel",26/09/2019,2019,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/abs/1909.11942,1.66E+03,Highly cited,Yes,2.89E+05,,,3.30E+09,,2.25E+10,,,,,,,,,,,Industry,
DistilBERT,Language,Text autocompletion,HuggingFace,Industry,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",02/10/2019,2019,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://arxiv.org/abs/1910.01108,8.95E+02,,No,6.60E+07,,,,,,,,,,,,Perplexity of next token,,,,Industry,
BART-large,Language,,Facebook AI,Industry,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",29/10/2019,2019,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://arxiv.org/abs/1910.13461,1.01E+03,Highly cited,Yes,4.06E+08,,,,,,,,,,,,,,,,Industry,
Noisy Student (L2),Vision,Image classification,"Carnegie Mellon University, Google",Industry - Academia Collaboration (Industry leaning),"Q Xie, MT Luong, E Hovy",11/11/2019,2019,Self-training with Noisy Student improves ImageNet classification,https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/,5.76E+02,,No,4.80E+08,,,,,1.04E+12,,,,,,,,,,,Industry,
MoCo,Drawing,Image completion,Facebook AI,Industry,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick",13/11/2019,2019,Momentum Contrast for Unsupervised Visual Representation Learning,https://arxiv.org/abs/1911.05722,1.72E+03,Highly cited,Yes,3.75E+08,,,,,,,,,,,,,,,,Industry,
,Vision,,University of Oxford,Academia,"S Wu, C Rupprecht, A Vedaldi",25/11/2019,2019,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,https://arxiv.org/abs/1911.11130,1.96E+02,,No,,,,,,,,,,,,,,,,,Academia,
StarGAN v2,Drawing,,"NAVER AI Lab, Yonsei University, Swiss Federal Institute of Technology ",Industry - Academia Collaboration (Industry leaning),"Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha",04/12/2019,2019,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/abs/1912.01865,8.32E+02,,No,,,,,,,,,,,,,,,,,Industry,
Big Transfer (BiT-L),Vision,Image classification,Google Research,Industry,"A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung",24/12/2019,2019,Large scale learning of general visual representations for transfer,https://arxiv.org/abs/1912.11370,8.30E+01,,No,9.28E+08,,,,,,,,,,,,,,,,Industry,
BigGAN-deep 512x512,Drawing,Image generation,"Heriot-Watt University, DeepMind",Industry - Academia Collaboration,"A Brock, J Donahue, K Simonyan",28/09/2018,2018,Large Scale GAN Training for High Fidelity Natural Image Synthesis,https://arxiv.org/abs/1809.11096,1.98E+03,Highly cited,Yes,1.13E+08,3.00E+21,JFT-300M,2.92E+08,,,,,,,,,,10448.44,,,Industry,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."
AmoebaNet-A (F=448),Vision,Image classification,Google Brain,Industry,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le",05/02/2018,2018,Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,1.71E+03,Highly cited,Yes,4.69E+08,3.85E+20,Imagenet-1k,1.28E+06,,,,,,,,,,5858.75,,,Industry,"The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures."
BERT-Large,Language,Next sentence prediction,Google AI,Industry,"J Devlin, MW Chang, K Lee, K Toutanova",11/10/2018,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805,2.38E+04,Highly cited,Yes,3.40E+08,2.85E+20,,3.30E+09,,7.90E+10,,,,,,,,999.93,Yes,,Industry,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
IMPALA,Games,Atari,DeepMind,Industry,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",05/02/2018,2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/abs/1802.01561,6.75E+02,,No,1.60E+06,1.68E+20,,2.40E+11,,,,,,,,,,2553.82,Yes,,Industry,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach."
YOLOv3,Vision,Object detection,University of Washington,Academia,"Joseph Redmon, Ali Farhadi",08/04/2018,2018,YOLOv3: An Incremental Improvement,https://arxiv.org/abs/1804.02767,7.71E+03,Highly cited,Yes,1.06E+08,5.09E+19,ImageNet,1.28E+06,,7.10E+10,,,,,,,,295.76,,,Academia,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL"
Population-based DRL,Games,Capture the flag,DeepMind,Industry,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",03/07/2018,2018,Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,https://arxiv.org/abs/1807.01281,4.34E+02,,No,1.22E+08,3.49E+19,,,,6.00E+10,,,,,,,,130.36,Yes,,Industry,"Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence."
GPT,Language,,OpenAI,Industry,"A Radford, K Narasimhan, T Salimans, I Sutskever",01/06/2018,2018,Improving Language Understanding by Generative Pre-Training,https://openai.com/blog/language-unsupervised/,2.26E+03,Highly cited,Yes,1.17E+08,1.76E+19,BooksCorpus,1.00E+09,,3.00E+10,,,,,,,,68.72,Yes,,Industry,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI)."
Refined Part Pooling,Vision,Person retrieval,"Tsinghua University, University of Technology Sydney, University of Texas at San Antonio",Academia,"Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang",09/01/2018,2018,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),https://arxiv.org/abs/1711.09349,1.24E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
ULM-FiT,Language,Text classification,"University of San Francisco, Insight Centre NUI Galway",Industry - Academia Collaboration,"J Howard, S Ruder",18/01/2018,2018,Universal Language Model Fine-tuning for Text Classification,https://arxiv.org/abs/1801.06146,1.94E+03,Highly cited,Yes,,,,,,,,,,,,,,,,LSTM RNN,Industry,
ELMo,Language,,"AllenAI, University of Washington",Industry,"ME Peters, M Neumann, M Iyyer, M Gardner",01/02/2018,2018,Deep contextualized word representations,https://arxiv.org/abs/1802.05365,7.48E+03,Highly cited,Yes,9.40E+07,,,,,2.60E+10,,,,,,,,,Yes,,Industry,
AmoebaNet-A (F=190),Vision,Image classification,Google Brain,Industry,"E Real, A Aggarwal, Y Huang, QV Le",05/02/2018,2018,Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,1.43E+03,Highly cited,Yes,8.70E+07,,,,,,,,,,,,,,,,Industry,
DeepLabV3+,Vision,Semantic segmentation,Google Inc.,Industry,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",07/02/2018,2018,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1802.02611v3,5.37E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image generation,"Preferred Networks Inc, Ritsumeikan University, National Institute of Informatics",Industry - Academia Collaboration,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida",16/02/2018,2018,Spectral Normalization for Generative Adversarial Networks,https://arxiv.org/abs/1802.05957,2.74E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image super-resolution,"Northeastern University, University of Rochester",Academia," Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu",24/02/2018,2018,Residual Dense Network for Image Super-Resolution,https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html,1.78E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Chinese - English translation,Language,Translation,Microsoft,Industry,"H Hassan, A Aue, C Chen, V Chowdhary",01/03/2018,2018,Achieving Human Parity on Automatic Chinese to English News Translation,https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/,5.30E+02,,No,,,,,,,,,,,,,,,Yes,,Industry,
Rotation,Drawing,Image completion,École des Ponts ParisTech,Academia,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",21/03/2018,2018,Unsupervised Representation Learning by Predicting Image Rotations,https://arxiv.org/abs/1803.07728,1.16E+03,Highly cited,Yes,8.60E+07,,,,,,,,,,,,,,,,Academia,
ResNeXt-101 32x48d,Vision,Image classification,Facebook,Industry,"D Mahajan, R Girshick",02/05/2018,2018,Exploring the Limits of Weakly Supervised Pretraining,https://arxiv.org/abs/1805.00932,6.19E+02,,No,8.29E+08,,,,,3.12E+10,,,,,,,,,,,Industry,
MobileNetV2,Vision,,Google Inc.,Industry,"M Sandler, A Howard, M Zhu",18/06/2018,2018,MobileNetV2: Inverted Residuals and Linear Bottlenecks,https://ieeexplore.ieee.org/document/8578572,5.71E+03,Highly cited,Yes,3.40E+06,,,,,6.00E+08,,,,,,,,,,,Industry,
ShuffleNet v2,Vision,,"Tsinghua University, Megvii Inc",Industry - Academia Collaboration,"N Ma, X Zhang, HT Zheng",30/06/2018,2018,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://arxiv.org/abs/1807.11164,1.41E+03,Highly cited,Yes,2.28E+06,,,,,3.00E+08,,,,,,,,,,,Industry,
,Vision,Image super-resolution,Northeastern University,Academia," Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu",08/07/2018,2018,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html,1.80E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
ESRGAN,Vision,Image super-resolution,"Chinese University of Hong Kong, Chinese Academy of Sciences, Nanyang Technological University",Academia,"Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang",01/09/2018,2018,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://arxiv.org/abs/1809.00219,1.50E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
MetaMimic,Games,,Google,Industry,"Tom Le Paine, Sergio Gomez",11/10/2018,2018,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,https://arxiv.org/abs/1810.05017,1.90E+01,,No,2.20E+07,,,,,,,,,,,,,,,,Industry,
GPipe (Amoeba),Vision,Image classification,Google,Industry,"Y Huang, Y Cheng, A Bapna, O Firat",16/11/2018,2018,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/abs/1811.06965,4.86E+02,,No,5.57E+08,,ImageNet,1.28E+06,,,,,,,,,,,,,Industry,
GPipe (Transformer),Language,Translation,Google,Industry,"Y Huang, Y Cheng, A Bapna, O Firat",16/11/2018,2018,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/abs/1811.06965,4.86E+02,,No,6.00E+09,,,2.0E+10,,,,,,,,,,,Yes,,Industry,
AlphaGo Zero,Games,Go,DeepMind,Industry,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",19/10/2017,2017,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,5.81E+03,Highly cited,Yes,4.64E+07,3.41E+23,,5.80E+09,,,,,,,,,,1544149.42,Yes,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved."
AlphaGo Master,Games,Go,DeepMind,Industry,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",01/01/2017,2017,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,5.81E+03,Highly cited,Yes,,1.50E+23,,,,,,,,,,,,852748.08,,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved."
AlphaZero,Games,,DeepMind,Industry,"D Silver, T Hubert, J Schrittwieser, I Antonoglou",05/12/2017,2017,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,https://arxiv.org/abs/1712.01815,1.08E+03,Highly cited,Yes,,3.67E+22,,7.00E+05,,,,,,,,,Score,162054.70,Yes,,Industry,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case."
Libratus,Games,Poker,Carnegie Mellon University,Academia,"N Brown, T Sandholm, S Machine",01/01/2017,2017,Libratus: The Superhuman AI for No-Limit Poker,https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf,9.70E+01,SOTA improvement,Yes,,1.15E+21,,,,,,3000000,,,,,,6253.49,,,Academia,"No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time."
OpenAI TI7 DOTA 1v1,Games,DOTA,OpenAI,Industry,"A Radford, K Narasimhan, T Salimans, I Sutskever",11/08/2017,2017,Dota 2 ,https://openai.com/five/,1.00E+03,,Yes,1.50E+08,6.05E+20,,,,,,,,,,,,2873.99,,,Industry,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task."
JFT,Vision,,"Google Research, CMU",Industry - Academia Collaboration,"ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta",04/08/2017,2017,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,https://arxiv.org/abs/1707.02968,1.14E+03,Highly cited,Yes,,4.79E+20,JFT-300M,3.00E+08,,,,,,,,,,21396.42,,,Industry,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets."
MoE,Language,Language modelling / Machine translation,"Google Brain, Jagiellonian University, Cracow",Industry - Academia Collaboration (Industry leaning),"N Shazeer, A Mirhoseini, K Maziarz, A Davis",23/01/2017,2017,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,6.87E+02,,No,8.70E+09,9.39E+19,,1.00E+11,,,,,,,,Sparse,,8484.35,,Long Short-Term Memory Mixture-Of-Experts,Industry,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
PNASNet-5,,,"Johns Hopkins University, Stanford, Google AI",Industry - Academia Collaboration (Industry leaning),"C Liu, B Zoph, M Neumann, J Shlens",02/12/2017,2017,Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,1.34E+03,Highly cited,Yes,,6.63E+19,Imagenet-1k,1.28E+06,,,,,,,,,,991.48,,,Industry,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet."
Transformer,Language,Translation,Google Brain ; Google Research,Industry,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",12/06/2017,2017,Attention Is All You Need,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,2.52E+04,Highly cited,Yes,2.13E+08,7.42E+18,,3.60E+08,,5.40E+10,,672,,,,,,111.17,Yes,,Industry,
DeepStack,Games,Poker,"University of Alberta, Charles University, Czech Technical University",Academia,"Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",06/01/2017,2017,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,https://arxiv.org/abs/1701.01724,6.18E+02,,No,2.50E+06,1.50E+14,,1.00E+07,,,,,,,,,,0.00,,,Academia,
,VIsion,Image super-resolution,"Harbin Institute of Technology, Hong Kong Polytechnic University, ULSee Inc., Xi’an Jiaotong University",Industry - Academia Collaboration (Academia leaning),"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",01/02/2017,2017,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://ieeexplore.ieee.org/abstract/document/7839189,4.00E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image classification,"University of Toronto, Twitter",Industry - Academia Collaboration," Jake Snell, Kevin Swersky, Richard S. Zemel",15/03/2017,2017,Prototypical Networks for Few-shot Learning,https://arxiv.org/abs/1703.05175,3.57E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Mask R-CNN,Vision,Image segmentation,Facebook AI Research,Industry,"Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",30/03/2017,2017,Mask R-CNN,https://arxiv.org/abs/1703.06870,1.50E+04,Highly cited,Yes,,,COCO,,,,,352,195,,,,,,,,Industry,
,Vision,Image generation,"Montreal Institute for learning Algorithms, Courant Institute of Mathematical Sciences",Academia,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",31/03/2017,2017,Improved Training of Wasserstein GANs,https://arxiv.org/abs/1704.00028,6.04E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
MobileNet,Vision,,Google Inc.,Industry,"AG Howard, M Zhu, B Chen, D Kalenichenko",17/04/2017,2017,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://arxiv.org/abs/1704.04861,9.19E+03,Highly cited,Yes,4.20E+06,,,,,1.14E+09,,,,,,,,,,,Industry,
,Vision,Image segmentation,"Google Inc., University King College, Johns Hopkins University",Industry - Academia Collaboration,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",27/04/2017,2017,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://ieeexplore.ieee.org/abstract/document/7913730,1.01E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image super-resolution,Twitter,Industry,"Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",25/05/2017,2017,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html,7.18E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Action recognition,"DeepMind, University of Oxford",Industry - Academia Collaboration,"Joao Carreira, Andrew Zisserman",01/06/2017,2017,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://arxiv.org/abs/1705.07750,3.98E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
PointNet++,,3D segmentation,Stanford University,Academia,"Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",07/06/2017,2017,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://arxiv.org/abs/1706.02413,4.02E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Image super-resolution,Seoul National University,Academia,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",10/06/2017,2017,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/abs/1707.02921,3.07E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
HRA,Games,Ms Pacman,Microsoft Maluuba,Industry - Academia Collaboration (Industry leaning),"H Van Seijen, M Fatemi, J Romoff, R Laroche",13/06/2017,2017,Hybrid Reward Architecture for Reinforcement Learning,https://arxiv.org/abs/1706.04208,2.22E+02,,No,,,,,,,,,,,,,,,,,Industry,
DeepLabV3,Vision,Semantic segmentation,Google Inc,Industry,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",17/06/2017,2017,Rethinking Atrous Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1706.05587,3.90E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image generation,Johannes Kepler University Linz,Academia,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, Sepp Hochreiter",26/06/2017,2017,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,https://arxiv.org/abs/1706.08500v1,4.17E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
NoisyNet-Dueling,Games,Atari Games,DeepMind,Industry,"M Fortunato, MG Azar, B Piot, J Menick",30/06/2017,2017,Noisy Networks for Exploration,https://arxiv.org/abs/1706.10295v3,4.80E+02,SOTA improvement,Yes,,,,,,,,,,,,,,,,,Industry,
ShuffleNet v1,Vision,,Megvii Inc,Industry,"X Zhang, X Zhou, M Lin, J Sun",03/07/2017,2017,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://arxiv.org/abs/1707.01083,2.78E+03,Highly cited,Yes,2.43E+06,,,,,1.40E+08,,,,,,,,,,,Industry,
NASNet-A,Vision,Image classification,Google Brain,Industry,"B Zoph, V Vasudevan, J Shlens",21/07/2017,2017,Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,3.10E+03,Highly cited,Yes,8.90E+07,,,,,,,,,,,,,,,,Industry,
,Vision,Image segmentation,Chinese University of Hong Kong,Industry - Academia Collaboration (Academia leaning),"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",21/07/2017,2017,Pyramid Scene Parsing Network,https://ieeexplore.ieee.org/document/8100143,6.07E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
RetinaNet-R50,Vision,Object detection,Facebook AI research,Industry,"TY Lin, P Goyal, R Girshick, K He",07/08/2017,2017,Focal loss for dense object detection,https://arxiv.org/abs/1708.02002,8.42E+03,Highly cited,Yes,3.40E+07,,,,,9.70E+10,,,,,,,,,,,Industry,
RetinaNet-R101,Vision,Object detection,Facebook AI research,Industry,"TY Lin, P Goyal, R Girshick, K He",07/08/2017,2017,Focal loss for dense object detection,https://arxiv.org/abs/1708.02002,8.42E+03,Highly cited,Yes,5.30E+07,,,,,1.27E+11,,,,,,,,,,,Industry,
,Vision,Image classification,"University of Guelph, Canadian Institute for Advanced Research and Vector Institute",Industry - Academia Collaboration," Terrance DeVries, Graham W. Taylor",15/08/2017,2017,Improved Regularization of Convolutional Neural Networks with Cutout,https://arxiv.org/abs/1708.04552,1.45E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
NeuMF (Pinterest),Recommendation,Collaborative filtering,"NUS, Columbia, Shandong University, Texas A&M",Academia,"X He, L Liao, H Zhang, L Nie, X Hu",16/08/2017,2017,Neural Collaborative Filtering,https://arxiv.org/abs/1708.05031,2.43E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
SENet (ImageNet),Vision,Image classification,Chinese Academy of Sciences ; University of Oxford,Academia,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu",05/09/2017,2017,Squeeze-and-Excitation Networks,https://arxiv.org/abs/1709.01507,7.94E+03,Highly cited,Yes,2.81E+07,,ImageNet,,,3.87E+09,,,,,,,,,,,Academia,
,Vision,Person re-identification,University of Technology Sydney,Academia,"Zhedong Zheng, Liang Zheng, Yi Yang",22/10/2017,2017,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,https://arxiv.org/abs/1701.07717,1.40E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
CapsNet (MNIST),Vision,Character recognition,Google Brain,Industry,"S Sabour, N Frosst, GE Hinton",26/10/2017,2017,Dynamic Routing Between Capsules,https://arxiv.org/abs/1710.09829,2.74E+03,Highly cited,Yes,8.20E+06,,,,,,,,,,,,,,,,Industry,
CapsNet (MultiMNIST),Vision,Character recognition,Google Brain,Industry,"S Sabour, N Frosst, GE Hinton",26/10/2017,2017,Dynamic Routing Between Capsules,https://arxiv.org/abs/1710.09829,2.74E+03,Highly cited,Yes,1.14E+07,,,,,,,,,,,,,,,,Industry,
,Vision,Image generation,Nvidia,Industry,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",27/10/2017,2017,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",https://arxiv.org/abs/1710.10196,3.91E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Person re-identification,"Visual Computing Institute, Aachen University",Academia,"Alexander Hermans, Lucas Beyer, Bastian Leibe",21/11/2017,2017,In Defense of the Triplet Loss for Person Re-Identification,https://arxiv.org/abs/1703.07737,1.99E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
PNAS-net,Vision,Image classification,"Johns Hopkins University, Stanford, Google AI",Industry - Academia Collaboration (Industry leaning),"C Liu, B Zoph, M Neumann, J Shlens",02/12/2017,2017,Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,1.15E+03,Highly cited,Yes,8.60E+07,,,,,,,,,,,,,,,,Industry,
GNMT,Language,Translation,Google,Industry,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",26/09/2016,2016,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://research.google/pubs/pub45610/,4.50E+03,Highly cited,Yes,2.78E+08,6.90E+21,,3.60E+08,,,,,,,,,,307573.50,,,Industry,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."
NASv3 (CIFAR-10),Vision,,Google Brain,Industry,"Barret Zoph, Quoc V. Le",05/11/2016,2016,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/abs/1611.01578,2.97E+03,Highly cited,Yes,3.74E+07,2.20E+21,,,39,,,,,,,,,13069.35,,,Industry,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214."
AlphaGo Lee,Games,Go,DeepMind,Industry,"D Silver, A Huang, CJ Maddison, A Guez, L Sifre",27/01/2016,2016,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature16961,1.08E+04,Highly cited,Yes,,1.90E+21,,2.94E+07,,,,,,,,,,14041.80,,,Industry,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away."
Xception,Vision,Image classification,Google,Industry,François Chollet,07/10/2016,2016,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/abs/1610.02357,5.84E+03,Highly cited,Yes,2.29E+07,4.36E+19,JFT,3.50E+08,,1.68E+10,,,,,,,,1961.34,,,Industry,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
Part-of-sentence tagging model,Language,POS tagging,University of Toronto,Academia,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton",21/07/2016,2016,Layer Normalization.,https://arxiv.org/abs/1607.06450,4.13E+03,Highly cited,Yes,,1.45E+17,,,,,,12,,,,,,0.97,,,Academia,
Named Entity Recognition model,Language,Named Entity Recognition model,University of Toronto,Academia,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton",21/07/2016,2016,Layer Normalization,https://arxiv.org/abs/1607.06450,4.13E+03,Highly cited,Yes,,9.69E+16,,,,,,8,,,,,,0.63,,,Academia,
R-FCN,Vision,Object detection,"Microsoft research, Tsinghua university",Industry - Academia Collaboration (Industry leaning),"Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",21/06/2016,2016,R-fcn: Object detection via region-based fully convolutional networks.,https://arxiv.org/abs/1605.06409,4.49E+03,Highly cited,Yes,,6.15E+16,PASCAL VOC (2007 and 2012 vesrions) + MS COCO,9.44E+04,,,,12.06567222,170,,,,,5.51,,,Industry,
,Vision,Pose estimation,"The Robotics Institute, Carnegie Mellon University",Academia,"Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",30/01/2016,2016,Convolutional Pose Machines,https://arxiv.org/abs/1602.00134,2.42E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
A3C FF hs,Games,Atari Games,"Google, University of Montreal",Industry - Academia Collaboration (Industry leaning),"V Mnih, AP Badia, M Mirza, A Graves",04/02/2016,2016,Asynchronous Methods for Deep Reinforcement Learning,http://arxiv.org/abs/1602.01783v2,5.28E+03,SOTA improvement,Yes,,,,,,,,,,,,,,,,,Industry,
Inceptionv4,Vision,Image classification,Google,Industry,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",23/02/2016,2016,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,8.21E+03,Highly cited,Yes,4.30E+07,,,,,2.46E+10,,,,,,,,,,,Industry,
Inception-ResNet-V2,Vision,Image classification,Google,Industry,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",23/02/2016,2016,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,8.21E+03,Highly cited,Yes,5.60E+07,,,,,2.64E+09,,,,,,,,,,,Industry,
SqueezeNet,Vision,Image classification,"DeepScale, UC Berkeley, Stanford",Industry - Academia Collaboration,"Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer",24/02/2016,2016,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,https://arxiv.org/abs/1602.07360,4.40E+03,Highly cited,Yes,1.20E+06,,,,,,,,,,,,,,,,Industry,
,Vision,Image super-resolution,"Nanjing University, University of Adelaide",Academia,"Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang",30/03/2016,2016,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/abs/1603.09056v2,1.18E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Video,"Graz University of Technology, University of Oxford",Academia,"Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",01/06/2016,2016,Convolutional Two-Stream Network Fusion for Video Action Recognition,https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html,2.28E+03,Highly cited,Yes,,,UCF101,9.72E+04,,,,,,,,,,,,,Academia,
DMN,Language,,Salesforce,Industry,"Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",20/06/2016,2016,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://arxiv.org/abs/1506.07285,1.19E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Recommendation,,Google,Industry,"HT Cheng, L Koc, J Harmsen, T Shaked",24/06/2016,2016,Wide & Deep Learning for Recommender Systems,https://arxiv.org/abs/1606.07792,1.61E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Language,,Facebook AI research,Industry,"A Joulin, E Grave, P Bojanowski, T Mikolov",06/07/2016,2016,Bag of Tricks for Efficient Text Classification,https://arxiv.org/abs/1607.01759,3.09E+03,Highly cited,Yes,,,,,,,,,,,,,,,,Continuous Bag-Of-Words Model,Industry,
,Language,,Facebook AI research,Industry,"P Bojanowski, E Grave, A Joulin",15/07/2016,2016,Enriching Word Vectors with Subword Information,https://arxiv.org/abs/1607.04606,6.35E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
DenseNet-264,Vision,Image classification,"Tsinghua University, Cornell, Facebook AI research",Industry - Academia Collaboration (Academia leaning),"G Huang, Z Liu, L Van Der Maaten",25/08/2016,2016,Densely Connected Convolutional Networks,https://arxiv.org/abs/1608.06993,1.78E+04,Highly cited,Yes,3.40E+07,,,,,,,,,,,,,,,,Industry,
,Vision,Face detection,"Chinese Academy of Sciences, Chinese University of Hong Kong",Academia,"Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao",26/08/2016,2016,Joint Face Detection and Alignment using Multitask cascaded convolutional networks,https://arxiv.org/abs/1604.02878,3.40E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Speech,,Google DeepMind,Industry,"A Oord, S Dieleman, H Zen, K Simonyan",12/09/2016,2016,WaveNet: A Generative Model for Raw Audio,https://arxiv.org/abs/1609.03499,3.12E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Recommendation,,Google,Industry,"Paul Covington, Jay Adams, and Emre Sargin",15/09/2016,2016,Deep Neural Networks for YouTube Recommendations,https://research.google/pubs/pub45530/,1.55E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Stacked hourglass network,Vision,Pose estimation,University of Michigan,Academia,"Alejandro Newell, Kaiyu Yang, Jia Deng",17/09/2016,2016,Stacked Hourglass Networks for Human Pose Estimation,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29,3.60E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",17/09/2016,2016,Identity Mappings in Deep Residual Networks,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,6.89E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Action recognition,"ETH Zurich, The Chinese University of Hong Kong, Shenzhen Institute of Advanced Technology",Academia,"Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",17/09/2016,2016,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,2.62E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
MS-CNN,Vision,Object detection,"SVCL UC San Diego, IBM",Industry - Academia Collaboration,"Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos",17/09/2016,2016,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22,1.32E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Wide Residual Network,Vision,Image classification,Université Paris-Est,Academia,"Sergey Zagoruyko, Nikos Komodakis",19/09/2016,2016,Wide Residual Networks,https://arxiv.org/abs/1605.07146,4.52E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Google DeepMind,Industry,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis",12/10/2016,2016,Hybrid computing using a neural network with dynamic external memory,https://www.nature.com/articles/nature20101,1.24E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image super-resolution,Seoul National University,Academia,"Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",11/11/2016,2016,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://arxiv.org/abs/1511.04491,1.97E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
ResNeXt-50,Vision,Image classification,"UC San Diego, Facebook",Industry - Academia Collaboration,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",16/11/2016,2016,Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,4.80E+03,Highly cited,Yes,2.50E+07,,,,,8.40E+09,,,,,,,,,,,Industry,
PolyNet,Vision,Image classification,The Chinese University of Hong Kong,Academia,"X Zhang, Z Li, C Change Loy",17/11/2016,2016,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,1.88E+02,,No,9.20E+07,,,,,,,,,,,,,,,,Academia,
RefineNet,Vision,Object detection,"University of Adelaide, Australian Centre for Robotic Vision",Industry - Academia Collaboration,"Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",20/11/2016,2016,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,https://arxiv.org/abs/1611.06612v3,2.06E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,UC Berkeley,Academia,"P Isola, JY Zhu, T Zhou",21/11/2016,2016,Image-to-Image Translation with Conditional Adversarial Networks,https://arxiv.org/abs/1611.07004,9.86E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,DeepMind,Industry,"J Kirkpatrick, R Pascanu",02/12/2016,2016,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/abs/1612.00796,2.16E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
PointNet,Other,3d segmentation,Stanford,Academia,"CR Qi, H Su, K Mo, LJ Guibas",02/12/2016,2016,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://arxiv.org/abs/1612.00593,5.04E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Image classification,OpenAI,Industry,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",05/12/2016,2016,Improved Techniques for Training GANs,https://dl.acm.org/doi/10.5555/3157096.3157346,6.06E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,,"UT Austin, Google Inc, UC Berkeley",Industry - Academia Collaboration,"V Gulshan, L Peng, M Coram, MC Stumpe, D Wu",13/12/2016,2016,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,https://jamanetwork.com/journals/jama/article-abstract/2588763,3.54E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
YOLOv2,Vision,Object detection,"University of Washington, Allen Institute for AI",Industry - Academia Collaboration,"Joseph Redmon, Ali Farhadi",25/12/2016,2016,"YOLO9000: Better, Faster, Stronger",https://arxiv.org/abs/1612.08242,9.37E+03,Highly cited,Yes,5.10E+07,,,,,,,,,,,,,,,,Industry,
AlphaGo Fan,Games,Go,Google DeepMind,Industry,"D Silver, A Huang, CJ Maddison, A Guez, L Sifre",01/10/2015,2015,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ,5.18E+03,SOTA improvement,Yes,8.21E+06,3.80E+20,,,,,,,,,,,,3076.07,,,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo."
DeepSpeech2,Speech,Speech recognition,Baidu Research- Silicon Valley AI Lab,Industry,"D Amodei, S Ananthanarayanan",08/12/2015,2015,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,https://arxiv.org/abs/1512.02595,2.21E+03,Highly cited,Yes,3.80E+07,2.60E+19,,9.80E+09,11,1.80E+09,,,,,,,,150.78,,,Industry,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale."
"MSRA (C, PReLU)",Vision,Image classification,Microsoft research,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",09/01/2015,2015,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/abs/1406.4729,1.41E+04,Highly cited,Yes,8.70E+07,2.40E+19,Imagenet-1k,1.28E+06,,,,,,,,,,2166.22,,,Industry,"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is ""artificial"" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, ""spatial pyramid pooling"", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition."
ResNet-152 (ImageNet),Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",10/12/2015,2015,Deep Residual Learning for Image Recognition,https://arxiv.org/abs/1512.03385,8.58E+04,Highly cited,Yes,6.00E+07,1.21E+19,ILSVRC 2012,1.20E+06,152,2.26E+10,,,,138,,,,92.03,,,Industry,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
GoogLeNet / InceptionV1,Vision,Image classification,"Google, University of Michigan, University of North Carolina",Industry - Academia Collaboration (Industry leaning),"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",07/06/2015,2015,Going deeper with convolutions,https://arxiv.org/abs/1409.4842,3.28E+04,Highly cited,Yes,6.80E+06,1.56E+18,ILSVRC 2014,1.20E+06,22,,,,,,,,,14.16,,,Industry,
,Vision,Image segmentation,"University of Oxford, Stanford University, Baidu",Industry - Academia Collaboration,"Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",11/02/2015,2015,Conditional Random Fields as Recurrent Neural Networks,https://arxiv.org/abs/1502.03240,2.66E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
DQN-2015,Games,Atari Games,Google,Industry,"V Mnih, K Kavukcuoglu, D Silver, AA Rusu, J Veness",25/02/2015,2015,Human-level control through deep reinforcement learning,https://www.nature.com/articles/nature14236,1.57E+04,Highly cited,Yes,1.62E+06,,,,,,,,,,,,,,,,Industry,
Constituency-Tree LSTM,Language,Semantic embedding,"Stanford, MetaMind Inc",Industry - Academia Collaboration,"KS Tai, R Socher, CD Manning",28/02/2015,2015,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://arxiv.org/abs/1503.00075,2.62E+03,Highly cited,Yes,2.05E+05,,,,,,,,,,,,,,,Tree-LSTM RNN,Industry,
Fast R-CNN,Vision,Object detection,Microsoft Research,Industry,R Girshick,30/04/2015,2015,Fast R-CNN,https://arxiv.org/abs/1504.08083,1.58E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Video,"University of Maryland, University of Texas, Google Inc.",Industry - Academia Collaboration,"Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici",01/05/2015,2015,Beyond Short Snippets: Deep Networks for Video Classification,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html,2.26E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
DSN,VIsion,Image classification,"University of California, Microsoft Research",Industry - Academia Collaboration (Academia leaning),"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",09/05/2015,2015,Deeply-Supervised Nets,http://proceedings.mlr.press/v38/lee15a.html,1.94E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Action recognition,"Chinese University of Hong Kong, Chinese Academy of Sciences",Academia,"Limin Wang, Yu Qiao, Xiaoou Tang",01/06/2015,2015,Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html,3.47E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Faster R-CNN,Vision,Object detection,Microsoft Research,Industry,"S Ren, K He, R Girshick, J Sun",04/06/2015,2015,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://arxiv.org/abs/1506.01497,2.30E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
YOLO,Vision,Object detection,University of Washington,Industry - Academia Collaboration,"J Redmon, S Divvala, R Girshick",08/06/2015,2015,"You Only Look Once: Unified, Real-Time Object Detection",https://arxiv.org/abs/1506.02640,1.75E+04,Highly cited,Yes,6.45E+07,,,,,,,,,,,,,,,,Industry,
BatchNorm,,,Google,Industry,"S Ioffe, C Szegedy",15/06/2015,2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,2.92E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,"Google, Carnegie Mellon University",Industry - Academia Collaboration,"William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals",20/08/2015,2015,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://ieeexplore.ieee.org/document/7472621,1.62E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
BPE,Language,Translation,University of Edinburgh,Academia,"R Sennrich, B Haddow, A Birch",31/08/2015,2015,Neural Machine Translation of Rare Words with Subword Units,https://arxiv.org/abs/1508.07909,4.06E+03,Highly cited,Yes,,,WMT'15,3.75E+07,,,,,,,,,,,,,Academia,
,,,Google DeepMind,Industry,"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",09/09/2015,2015,Continuous control with deep reinforcement learning,https://arxiv.org/abs/1509.02971,6.35E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,Google DeepMind,Industry,"Z Wang, T Schaul, M Hessel",20/11/2015,2015,Dueling Network Architectures for Deep Reinforcement Learning,https://arxiv.org/abs/1511.06581,2.00E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image segmentation,"Princeton University, Intel Labs",Industry - Academia Collaboration,"Fisher Yu, Vladlen Koltun",23/11/2015,2015,Multi-Scale Context Aggregation by Dilated Convolutions,https://arxiv.org/abs/1511.07122,5.84E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Recommendation,,Netflix,Industry,"CA Gomez-Uribe, N Hunt",01/12/2015,2015,"The Netflix Recommender System: Algorithms, Business Value, and Innovation",https://dl.acm.org/doi/pdf/10.1145/2843948,1.09E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Inception v3,Vision,Image classification,"Google, University College London",Industry - Academia Collaboration (Industry leaning),"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",02/12/2015,2015,Rethinking the inception architecture for computer vision.,https://arxiv.org/abs/1512.00567,1.47E+04,Highly cited,Yes,2.36E+07,,ILSVRC 2012,1.20E+06,,1.15E+11,,,,,,,Score,,,,Industry,
ResNet-110 (CIFAR-10),Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",10/12/2015,2015,Deep Residual Learning for Image Recognition,https://arxiv.org/abs/1512.03385,8.58E+04,Highly cited,Yes,1.70E+06,,,,110,,,,,,,,,,,,Industry,
BPL,Drawing,Image generation,"NYU, University of Toronto, MIT",Academia,"BM Lake, R Salakhutdinov, JB Tenenbaum",11/12/2015,2015,Human-level concept learning through probabilistic program induction,https://science.sciencemag.org/content/350/6266/1332/,2.01E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Advantage Learning,Games,Atari Games,Google DeepMind,Industry,"MG Bellemare, G Ostrovski, A Guez",15/12/2015,2015,Increasing the Action Gap: New Operators for Reinforcement Learning,http://arxiv.org/abs/1512.04860v1,1.04E+02,SOTA improvement,Yes,,,,,,,,,,,,,,,,,Industry,
VGG16,Vision,,University of Oxford,Academia,Karen Simonyan; Andrew Zisserman,04/09/2014,2014,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,6.13E+04,Highly cited,Yes,1.38E+08,8.52E+18,ILSVRC-2012,1.30E+06,16,1.53E+10,,,,,,,,82.80,,,Academia,
Seq2Seq LSTM,Language,Translation,Google,Industry,"I Sutskever, O Vinyals, QV Le",10/09/2014,2014,Sequence to Sequence Learning with Neural Networks,https://arxiv.org/abs/1409.3215,1.57E+04,Highly cited,Yes,3.84E+08,7.30E+18,WMT'14 dataset,3.84E+08,,,,,,,,,,79.60,,,Industry,
SPPNet,Vision,Image classification,"Microsoft, Xi’an Jiaotong University, University of Science and Technology of China",Industry - Academia Collaboration,,18/06/2014,2014,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/abs/1406.4729,7.41E+03,Highly cited,Yes,,6.11E+18,Imagenet-1k,1.28E+06,,,,,,,,,,65.07,,,Industry,
RNNsearch-50*,Language,Translation,"Universite de Montréal, Jacobs University Bremen",Academia,"D Bahdanau, K Cho, Y Bengio",01/09/2014,2014,Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/abs/1409.0473,1.92E+04,Highly cited,Yes,,1.56E+18,WMT'14 + selection,3.84E+08,,,,,,,,,,81.48,,,Academia,
GANs,Drawing,Image generation,Universite de Montréal,Academia,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",10/06/2014,2014,Generative Adversarial Networks,https://arxiv.org/abs/1406.2661,3.69E+04,Highly cited,Yes,,5.18E+17,CIFAR-10,6.00E+04,,,,,,,,,,6.09,,,Academia,
ADAM (CIFAR-10),Vision,Image classification,"University of Amsterdam, OpenAI, University of Toronto",Industry - Academia Collaboration,"DP Kingma, J Ba",22/12/2014,2014,Adam: A Method for Stochastic Optimization,https://arxiv.org/abs/1412.6980,8.11E+04,Highly cited,Yes,,6.05E+16,,,,,,,,,,,,0.60,,,Industry,
GloVe (6B),Language,Semantic embedding,Stanford,Academia,"J Pennington, R Socher, CD Manning",01/01/2014,2014,GloVe: Global Vectors for Word Representation,https://nlp.stanford.edu/projects/glove/,2.25E+04,Highly cited,Yes,1.20E+08,,Gigaword5 + Wikipedia2014,6.00E+09,,N/A,,12.1875,0,35,,,,,,Regression Model,Academia,
GloVe (32B),Language,Semantic embedding,Stanford,Academia,"J Pennington, R Socher, CD Manning",01/01/2014,2014,GloVe: Global Vectors for Word Representation,https://nlp.stanford.edu/projects/glove/,2.25E+04,Highly cited,Yes,1.20E+08,,Common Crawl,4.20E+10,,N/A,,12.1875,0,35,,,,,,Regression Model,Academia,
DBNs,Language,Text classification,"Microsoft, University of Toronto",Industry - Academia Collaboration,"R Sarikaya, GE Hinton, A Deoras",11/02/2014,2014,Application of Deep Belief Networks for Natural Language Understanding,https://ieeexplore.ieee.org/document/6737243,4.45E+02,,No,1.02E+06,,,1.78E+05,3,,,,,,,,,,,,Industry,
HyperNEAT,Games,Atari Games,University of Texas,Academia,"M Hausknecht, J Lehman",05/03/2014,2014,A Neuroevolution Approach to General Atari Game Playing,https://ieeexplore.ieee.org/abstract/document/6756960,1.95E+02,,No,2.40E+05,,,,,,,,,,,,,,,,Academia,
,,,University of Toronto,Academia,"Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky",01/06/2014,2014,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,https://jmlr.org/papers/v15/srivastava14a.html,3.19E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
GRUs,Language,,"University of Montreal, Jacobs University, University du Maine",Academia,"K Cho, B Van Merriënboer, C Gulcehre",03/06/2014,2014,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,https://arxiv.org/abs/1406.1078,1.50E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Video,Video classification,University of Oxford,Academia,"Karen Simonyan, Andrew Zisserman",09/06/2014,2014,Two-Stream Convolutional Networks for Action Recognition in Videos,https://arxiv.org/abs/1406.2199,6.22E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Multiresolution CNN,Video,Video classification,"Stanford, Google",Industry - Academia Collaboration,"A Karpathy, G Toderici, S Shetty, T Leung",23/06/2014,2014,Large-Scale Video Classification with Convolutional Neural Networks,https://ieeexplore.ieee.org/document/6909619,5.90E+03,Highly cited,Yes,1.26E+08,,,,,,,,,,,,,,,,Industry,
,Vision,Face verification,"Tel Aviv University, Facebook",Industry - Academia Collaboration,"Y Taigman, M Yang, MA Ranzato",23/06/2014,2014,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,https://ieeexplore.ieee.org/document/6909616,5.74E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
SmooCT,Games,,University College London,Academia,"Johannes Heinrich, David Silver",01/07/2014,2014,Self-Play Monte-Carlo Tree Search in Computer Poker,https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7,1.60E+01,SOTA improvement,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Facebook,Industry,"X He, J Pan, O Jin, T Xu, B Liu, T Xu, Y Shi",24/08/2014,2014,Practical Lessons from Predicting Clicks on Ads at Facebook,https://dl.acm.org/doi/10.1145/2648584.2648589,5.86E+02,,No,,,,,,,,,,,,,,,,,Industry,
VGG19,Vision,,University of Oxford,Academia,"K Simonyan, A Zisserman",04/09/2014,2014,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,6.13E+04,Highly cited,Yes,1.44E+08,,ILSVRC-2012,1.30E+06,19,1.96E+10,,,,,,,,,,,Academia,
LRCN,Vision,Video description,"UT Austin, UMass Lowell, UC Berkeley",Academia,"Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell",07/11/2014,2014,Long-term Recurrent Convolutional Networks for Visual Recognition and Description,https://arxiv.org/abs/1411.4389,5.22E+03,Highly cited,Yes,1.43E+08,,TaCoS,4.00E+04,,,,,,,Reinforcement learning,,,,,,Academia,
,Vision,Image segmentation,"University of California, Berkeley",Academia,"J Long, E Shelhamer, T Darrell",14/11/2014,2014,Fully Convolutional Networks for Semantic Segmentation,https://arxiv.org/abs/1411.4038,2.47E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,The Chinese University of Hong Kong,Academia,"Z Liu, P Luo, X Wang, X Tang",28/11/2014,2014,Deep Learning Face Attributes in the Wild,https://arxiv.org/abs/1411.7766,4.01E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
NTM,,,Google DeepMind,Industry,"Alex Graves, Greg Wayne, Ivo Danihelka",10/12/2014,2014,Neural Turing Machines,https://arxiv.org/abs/1410.5401,1.93E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
DeepLab,Vision,Image segmentation,"Google, University of California Los Angeles",Industry - Academia Collaboration,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",22/12/2014,2014,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,https://arxiv.org/abs/1412.7062,3.70E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
TransE,Other,Entity embedding,"CNRS, Google",Industry - Academia Collaboration,"Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",05/12/2013,2013,Translating Embeddings for Modeling Multi- relational Data,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,4.00E+03,Highly cited,Yes,,1.34E+18,,1.70E+07,,,,,,,,,,17.58,,,Industry,
Visualizing CNNs,Vision,,NYU,Academia,"MD Zeiler, R Fergus",12/11/2013,2013,Visualizing and Understanding Convolutional Networks,https://arxiv.org/abs/1311.2901,1.30E+04,Highly cited,Yes,,5.32E+17,,,,,,,,,,,Predict nearby words,9.02,,,Academia,
Mitosis,Vision,,IDSIA,Academia,"Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber",22/09/2013,2013,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51,1.46E+03,ICPR 2012 mitosis detection competition winner,Yes,3.72E+04,1.37E+17,,1.00E+06,,,,,,,,,,2.00,,,Academia,
Word2Vec (large),Language,Semantic embedding,Google,Industry,"T Mikolov, I Sutskever, K Chen, GS Corrado",16/10/2013,2013,Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/abs/1310.4546,2.87E+04,Highly cited,Yes,6.92E+09,3.89E+16,,6.92E+05,,,,,,,,,Predict nearby words,0.55,,Recurrent Neural Network,Industry,
DQN,Games,Atari,DeepMind,Industry,"V Mnih, K Kavukcuoglu, D Silver, A Graves",01/01/2013,2013,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,6.68E+03,Highly cited,Yes,8.36E+05,2.30E+15,,,,,,,,,,,,0.04,,,Industry,
Image generation,Vision,Image clustering,Univeristy of Amsterdam,Academia,"DP Kingma, M Welling",20/12/2013,2013,Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,1.56E+04,Highly cited,Yes,,4.75E+14,MNIST,6.00E+04,,,,,,,,,,0.01,,,Academia,
Image Classification with the Fisher Vector: Theory and Practice,Vision,Image Classification,"Universidad Nacional de Cordoba, Xerox Research Centre Europe, Inteligent Systems Lab Amsterdam, University of Amsterdam, LEAR Team, INRIA Grenoble",Industry - Academia Collaboration,"orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek",12/06/2013,2013,Image Classification with the Fisher Vector: Theory and Practice,https://hal.inria.fr/hal-00830491v2/document,1.71E+03,Highly cited,Yes,,9.08E+13,ImageNet,,,,2,,,,,,,0.00,,,,
,Vision,,Stanford,Academia,"R Socher, M Ganjoo, H Sridhar, O Bastani",16/01/2013,2013,Zero-Shot Learning Through Cross-Modal Transfer,https://arxiv.org/abs/1301.3666,1.21E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Maxout Networks ,Vision,Image classification,University of Montreal,Academia," Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",18/02/2013,2013,Maxout Networks ,https://arxiv.org/abs/1302.4389,2.58E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
PreTrans-3L-250H,Speech,Speech recognition,Univeristy of Toronto,Academia,"Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton",22/03/2013,2013,Speech Recognition with Deep Recurrent Neural Networks,https://arxiv.org/abs/1303.5778,7.79E+03,Highly cited,Yes,4.30E+07,,,,,,,,,,,,,,,,Academia,
,Vision,Object detection,"Univeristy of Trento, University of Amsterdam",Academia,"JRR Uijlings, KEA Van De Sande, T Gevers",02/04/2013,2013,Selective search for object recognition,https://link.springer.com/article/10.1007/s11263-013-0620-5,5.59E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,"University of Trento, University of Amsterdam",Academia,"JRR Uijlings, KEA Van De Sande, T Gevers",02/04/2013,2013,Selective search for object recognition,https://link.springer.com/article/10.1007/s11263-013-0620-5,5.59E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Language,,Microsoft Research,Industry,"T Mikolov, W Yih, G Zweig",09/06/2013,2013,Linguistic Regularities in Continuous Space Word Representations,https://www.aclweb.org/anthology/N13-1090/,3.63E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Word2Vec (small),Language,Semantic embedding,Google,Industry,"T Mikolov, I Sutskever, K Chen, GS Corrado",16/10/2013,2013,Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/abs/1310.4546,2.87E+04,Highly cited,Yes,2.08E+08,,,6.92E+05,,,,,,,,,,,,Recurrent Neural Network,Industry,
R-CNN (T-net),Vision,Object detection,UC Berkeley,Academia,"Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",11/11/2013,2013,Rich feature hierarchies for accurate object detection and semantic segmentation,https://arxiv.org/abs/1311.2524,1.91E+04,Highly cited,Yes,6.90E+07,,,,,,,,,,,,,,,,Academia,
,,,Stanford,Academia,"R Socher, D Chen, CD Manning, A Ng",01/12/2013,2013,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html,1.66E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
DBLSTM,Speech,Speech recognition,Univeristy of Toronto,Academia,"A Graves, N Jaitly, A Mohamed",08/12/2013,2013,Hybrid speech recognition with Deep Bidirectional LSTM,https://ieeexplore.ieee.org/document/6707742,1.46E+03,Highly cited,Yes,2.99E+07,,,,,,,,,,,,,,,,Academia,
Network in Network,,,National University of Singapore,Academia,"M Lin, Q Chen, S Yan",16/12/2013,2013,Network In Network,https://arxiv.org/abs/1312.4400,5.50E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Image classification,New York University,Academia,"Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",21/12/2013,2013,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",https://arxiv.org/abs/1312.6229,5.15E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
AlexNet,Vision,Image classification,University of Toronto,Academia,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",30/09/2012,2012,ImageNet Classification with Deep Convolutional Neural Networks,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,8.51E+04,Highly cited,Yes,6.00E+07,4.70E+17,ImageNet,1.20E+06,8,,,,,,,,,8.00,,,Academia,
Dropout (MNIST),Vision,Character recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",03/06/2012,2012,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,Yes,5.59E+06,6.04E+15,MNIST,6.00E+04,2,1.12E+07,,,,0.021,,,,0.10,,,Academia,
MCDNN (MNIST),Vision,Character recognition,IDSIA,Academia,"D Ciregan, U Meier, J Schmidhuber",13/02/2012,2012,Multi-column Deep Neural Networks for Image Classification,https://arxiv.org/abs/1202.2745v1,4.83E+03,Highly cited,Yes,1.99E+06,3.73E+15,MNIST,6.00E+04,3,2.59E+07,,490,,0.021,,,,0.08,,,Academia,
Dropout (TIMIT),Speech,Speech recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",03/06/2012,2012,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,Yes,4.88E+07,,TIMIT,4.16E+04,4,,,,,,,,,,,,Academia,
Dropout (CIFAR),Vision,Character recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",03/06/2012,2012,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Dropout (ImageNet),Vision,Image classification,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",03/06/2012,2012,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,Yes,,,ImageNet,1.00E+06,7,,,,,,,,,,,,Academia,
,,,"Karlsruhe Institute of Technology, Toyota Technological Institute at Chicago",Industry - Academia Collaboration,"A Geiger, P Lenz, R Urtasun",16/06/2012,2012,Are we ready for autonomous driving? The KITTI vision benchmark suite,http://www.cvlibs.net/publications/Geiger2012CVPR_slides.pdf,7.14E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Language,,Google,Industry,"Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov",08/07/2012,2012,Syntactic Annotations for the Google Books NGram Corpus,https://aclanthology.org/P12-3029/,4.89E+02,,No,,,,,,,,,,,,,,,,,Industry,
MV-RNN,Language,Text classification,Stanford,Academia,"R. Socher, B. Huval, C. D. Manning, and A. Y. Ng",12/07/2012,2012,Semantic compositionality through recursive matrix-vector spaces,https://www.aclweb.org/anthology/D12-1110/,1.46E+03,Highly cited,Yes,3.51E+06,,,,,,,,,,,,,,,Matrix-Vector RNN,Academia,
,,,"University of Toronto, University of Sherbrooke, Harvard University",Academia,"J Snoek, H Larochelle, RP Adams",02/12/2012,2012,Practical Bayesian optimization of machine learning algorithms,https://arxiv.org/abs/1206.2944,5.67E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,"University of Michigan, Stanford University",Academia,"A Coates, A Ng, H Lee",11/04/2011,2011,An analysis of single-layer networks in unsupervised feature learning,http://proceedings.mlr.press/v15/coates11a.html,2.66E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,University of Montreal,Academia,"X Glorot, A Bordes, Y Bengio",13/04/2011,2011,Deep sparse rectifier neural networks,http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf,7.22E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Language,,"Brno University of Technology, Johns Hopkins University",Academia,"T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur",22/05/2011,2011,Extensions of recurrent neural network language model,https://ieeexplore.ieee.org/document/5947611,1.24E+03,Highly cited,Yes,,,Penn Tree Bank,6.98E+05,,,,,,,,,,,,Recurrent Neural Network,Academia,
,Language,Part-of-speech tagging,"Carnegie Mellon University, Google Research",Industry - Academia Collaboration,"Dipanjan Das, Slav Petrov",19/06/2011,2011,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,https://aclanthology.org/P11-1061/,3.16E+02,,No,,,,,,,,,,,,,,,,,Industry,
,Language,,Stanford,Academia,"R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning",01/07/2011,2011,Semi-supervised recursive autoencoders for predicting sentiment distributions,https://aclanthology.org/D11-1014/,1.48E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Bayesian Starcraft,Games,,Collège de France,Academia,"G Synnaeve, P Bessiere",31/08/2011,2011,A Bayesian Model for RTS Units Control applied to StarCraft,https://ieeexplore.ieee.org/document/6032006,8.60E+01,,No,1.31E+04,,,,,,,,,,,,,,,,Academia,
,,,"Univeristy of California Berkley, Technion- Israel Institute of Technology, Google",Industry - Academia Collaboration,"J Duchi, E Hazan, Y Singer",03/10/2011,2011,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,https://dl.acm.org/doi/10.5555/1953048.2021068,8.81E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Domain Adaptation,Vision,Object Recognition,"University of Maryland, College Park",Academia,"Raghuraman Gopalan, Ruonan Li, Rama Chellappa",06/11/2011,2011,Domain Adaptation for Object Recognition: An Unsupervised Approach,http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf,1.06E+03,Highly cited,Yes,1.53E+04,,Dataset introduced in 'Adapting Visual Category Models to New Domains',4.65E+03,,,,,,,Supervised,,,,,,Academia,
NLP from scratch,Language,,"NEC Laboratories, Princeton",Industry - Academia Collaboration,"Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa",08/11/2011,2011,Natural Language Processing (Almost) from Scratch,https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf,7.64E+03,Highly cited,Yes,5.00E+06,,,8.52E+08,,,,,,,,,,,,,Industry,
,,,University of Wisconsin Madison,Academia,"F Niu, B Recht, C Ré, SJ Wright",11/11/2011,2011,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,https://arxiv.org/abs/1106.5730,2.12E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
KN5 LM + RNN 400/10 (WSJ),Speech,Transcription,"Brno University of Technology, Johns Hopkins University",Academia,"T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",26/09/2010,2010,Recurrent neural network based language model.,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5.67E+03,Highly cited,Yes,8.00E+07,6.14E+16,WSJ,6.40E+06,,1.60E+08,,,,,,,,2.03,,,Academia,
RNN 500/10 + RT09 LM (NIST RT05),Speech,Transcription,"Brno University of Technology, Johns Hopkins University",Academia,"T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",26/09/2010,2010,Recurrent neural network based language model.,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5.67E+03,Highly cited,Yes,5.27E+06,3.41E+15,NIST RT05,5.40E+06,,1.05E+07,,,,,,,,0.11,,,Academia,
Feedforward NN,Vision,Digit recognition,University of Montreal,Academia,"X Glorot, Y Bengio",13/05/2010,2010,Understanding the difficulty of training deep feedforward neural networks,https://proceedings.mlr.press/v9/glorot10a.html,1.33E+04,Highly cited,Yes,7.08E+06,3.50E+14,MNIST,,,1.40E+07,,,,,,,,0.01,,,Academia,
6-layer MLP (MNIST),Vision,Character recognition,IDSIA ; University of Lugano & SUPSI,Academia,"Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber",01/03/2010,2010,Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition,https://arxiv.org/abs/1003.0358,1.26E+03,Highly cited,Yes,1.21E+07,1.31E+14,MNIST,6.00E+04,,,,,,,,,,0.01,,,Academia,
,,,"University of Montreal, University of Toronto",Academia,"P Vincent, H Larochelle, I Lajoie, Y Bengio",03/01/2010,2010,Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,https://www.jmlr.org/papers/v11/vincent10a.html,6.23E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Word Representations,Language,,"University of Montreal, University of Illinois at Urbana- Champaigne",Academia,"Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio",01/06/2010,2010,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://aclanthology.org/P10-1040.pdf,2.51E+03,Highly cited,Yes,,,,3.70E+07,,,,,,,,,,,,,Academia,
,Vision,,"INRIA, Ecole, NYU",Academia,"YL Boureau, F Bach, Y LeCun",13/06/2010,2010,Learning mid-level features for recognition,https://ieeexplore.ieee.org/document/5539963,1.31E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Deconvolutional Network,Vision,,NYU,Academia,"Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus",13/06/2010,2010,Deconvolutional Networks,https://ieeexplore.ieee.org/document/5539957,1.52E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
ReLU (NORB),Vision,Object recognition,University of Toronto,Academia,"Nair, V., Hinton, G. E.",15/06/2010,2010,Rectified linear units improve restricted boltzmann machines,https://dl.acm.org/doi/10.5555/3104322.3104425,1.40E+04,Highly cited,Yes,1.62E+07,,,2.92E+05,,,,,,,,,,,,,Academia,
ReLU (LFW),Vision,Face recognition,University of Toronto,Academia,"Nair, V., Hinton, G. E.",15/06/2010,2010,Rectified linear units improve restricted boltzmann machines,https://dl.acm.org/doi/10.5555/3104322.3104425,1.40E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,University of Toronto,Academia,GE Hinton,02/08/2010,2010,A practical guide to training restricted boltzmann machines,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32,3.34E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,Xerox Research Centre Europe (XRCE),Industry,Florent PerronninJorge SánchezThomas Mensink,05/09/2010,2010,Improving the Fisher Kernel for Large-Scale Image Classification,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11,3.06E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Recommendation,,Google Inc,Industry,"J Davidson, B Liebald, J Liu, P Nandy",26/09/2010,2010,The YouTube Video Recommendation System,https://dl.acm.org/doi/10.1145/1864708.1864770,1.07E+03,Highly cited,Yes,,,,1.00E+10,,,,,,,,,,,,,Industry,
,Language,,Harvard,Industry - Academia Collaboration,"JB Michel, YK Shen, AP Aiden, A Veres, MK Gray",16/12/2010,2010,Quantitative Analysis of Culture Using Millions of Digitized Books,https://science.sciencemag.org/content/331/6014/176,2.27E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
GPU DBNs,Other,,Stanford,Academia,"R Raina, A Madhavan, AY Ng",15/06/2009,2009,Large-scale Deep Unsupervised Learning using Graphics Processors,http://www.machinelearning.org/archive/icml2009/papers/218.pdf,7.89E+02,,No,1.00E+08,1.00E+15,,1.00E+06,,,,,,,,,,0.06,,,Academia,
Deep Boltzmann Machines,,,University of Toronto,Academia,"Ruslan Salakhutdinov, Geoffrey Hinton",16/04/2009,2009,Deep Boltzmann Machines,https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine,2.67E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Stanford,Academia,"H Lee, R Grosse, R Ranganath, AY Ng",14/06/2009,2009,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/10.1145/1553374.1553453,2.96E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
BellKor 2008,Recommendation,Movie ratings,AT&T Labs,Industry,"RM Bell, Y Koren, C Volinsky",01/08/2009,2009,The BellKor 2008 Solution to the Netflix Prize,https://www.researchgate.net/publication/228766792_The_BellKor_2008_solution_to_the_Netflix_Prize,1.58E+02,,No,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
RL mapping instructions (games),Reading,Instruction interpretation,MIT,Academia,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",01/08/2009,2009,Reinforcement Learning for Mapping Instructions to Actions,https://aclanthology.org/P09-1010/,2.95E+02,,No,8.09E+04,,Windows Help and Support,2.93E+02,,,,,,,,,,,,Log-linear model?,Academia,
,Recommendation,Movie ratings,AT&T Labs,Industry,Y Koren,01/08/2009,2009,The BellKor Solution to the Netflix Grand Prize,https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf,5.07E+02,,No,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
,Recommendation,Movie ratings,AT&T Labs,Industry,"A Töscher, M Jahrer, RM Bell",01/08/2009,2009,The BigChaos Solution to the Netflix Grand Prize,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf,2.37E+02,,No,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
,Recommendation,Movie ratings,Pragmatic Theory Inc.,Industry,"M Piotte, M Chabbert",01/08/2009,2009,The Pragmatic Theory solution to the Netflix Grand Prize,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf,1.11E+02,,No,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
RL mapping instructions (troubleshooting),Reading,Instruction interpretation,MIT,Academia,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",02/08/2009,2009,Reinforcement Learning for Mapping Instructions to Actions,https://aclanthology.org/P09-1010/,2.95E+02,,No,1.33E+05,,Windows Help and Support,1.33E+03,,,,,,,,,,,,Log-linear model?,Academia,
,Recommendation,,"AT&T Labs, Yahoo Research",Industry,"Yehuda Koren, Robert Bell, and Chris Volinsky",07/08/2009,2009,Matrix factorization techniques for recommender systems,https://ieeexplore.ieee.org/document/5197422,8.91E+03,Highly cited,Yes,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
,Language,,"University of Edinburgh, University of Pittsburgh",Academia,"Theresa Wilson, Janyce Wiebe, Paul Hoffmann.",01/09/2009,2009,Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis,https://aclanthology.org/J09-3003.pdf,7.87E+02,,No,,,,1.11E+04,,,,,,,,,,,,,Academia,
BellKor 2007,Recommendation,Movie ratings,AT&T Labs,Industry,"RM Bell, Y Koren, C Volinsky",21/09/2009,2009,The BellKor solution to the Netflix Prize,https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba,2.38E+02,,No,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
,3D reconstruction,,"University of Washington, Cornell, Microsoft Research",Industry - Academia Collaboration (Academia leaning),"Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski",29/09/2009,2009,Building Rome in a Day,https://grail.cs.washington.edu/rome/,2.20E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Language,,University of Colorado & New Mexico State University,Academia,"Thomas K Landauer, Peter W. Foltz & Darrell Laham",11/11/2009,2009,An Introduction to Latent Semantic Analysis,https://www.tandfonline.com/doi/abs/10.1080/01638539809545028,6.42E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,"University of Montreal, Microsoft Research",Academia,Y Bengio,15/11/2009,2009,Learning deep architectures for AI,https://www.nowpublishers.com/article/Details/MAL-006,9.78E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,"University of Chicago & Toyota Technological Institute, Chicago & University of California, Irvine",Academia,"Pedro Felzenszwalb, David McAllester, Deva Ramanan",23/06/2008,2008,"A discriminatively trained, multiscale, deformable part model",https://ieeexplore.ieee.org/abstract/document/4587597,3.09E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Denoising Autoencoders,,,University of Montreal,Academia,"Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",05/07/2008,2008,Extracting and Composing Robust Features with Denoising Autoencoders,https://dl.acm.org/doi/10.1145/1390156.1390294,6.30E+03,,Yes,,,,,,,,,,,,,,,,,Academia,
,,,"Google, NUANCE Communications, UIUC, IDIAP",Industry - Academia Collaboration,"Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert",05/07/2008,2008,Deep Learning via Semi-Supervised Embedding,https://dl.acm.org/doi/10.1145/1390156.1390303,1.09E+03,,Yes,,,,,,,,,,,,,,,,,Industry,
,Language,,NEC Laboratories,Industry,"R Collobert, J Weston",05/07/2008,2008,A unified architecture for natural language processing: Deep neural networks with multitask learning,https://dl.acm.org/doi/10.1145/1390156.1390177,5.76E+03,Highly cited,Yes,,,,6.31E+08,,,,,,,,,,,,Convolutional Neural Network,Industry,
Stacked Semisuperviser Autoencoders,Language,Document representation,New York University and Microsoft Research Cambridge,Industry - Academia Collaboration,"MA Ranzato, M Szummer",15/07/2008,2008,Semisupervised learning of compact document representations with deep networks,https://dl.acm.org/doi/10.1145/1390156.1390256,2.43E+02,,No,3.00E+06,,,6.61E+04,,,,,,,,,,,,,Industry,
Boss (DARPA Urban Challenge),Driving,Self-driving car,Carnegie Mellon University,Industry - Academia Collaboration,"Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar",23/07/2008,2008,Autonomous Driving in UrbanEnvironments: Boss and theUrban Challenge,https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255,1.84E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,,"Univeristy of Lubeck, Germany",Academia,"Kai Labusch, Erhadt Barth, Thomas Martinetz",19/11/2008,2008,Simple method for high-performance digit recognition based on sparse coding,https://pubmed.ncbi.nlm.nih.gov/19000969/,1.24E+02,,No,,,,,,,,,,,,,,,,,Academia,
BigChaos 2008,Recommendation,Movie ratings,AT&T Labs,Industry,"A Töscher, M Jahrer",25/11/2008,2008,The BigChaos Solution to the Netflix Prize 2008,https://www.researchgate.net/publication/228419683_The_bigchaos_solution_to_the_netflix_prize_2008,3.50E+01,Historical relevance,Yes,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Industry,
Semantic Hashing,Other,,University of Toronto,Academia,"R Salakhutdinov, G Hinton",10/12/2008,2008,Semantic Hashing,https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf,1.49E+03,Highly cited,Yes,2.60E+06,,,3.11E+05,,,,,,,,,,,,,Academia,
,Language,,University of Edinburgh,Academia,"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch",01/06/2007,2007,Moses: Open Source Toolkit for Statistical Machine Translation,https://aclanthology.org/P07-2045.pdf,6.10E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
λ-WASP,Language,,UT Austin,Academia,"YW Wong, R Mooney",01/06/2007,2007,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://www.aclweb.org/anthology/P07-1121/,3.83E+02,,No,4.90E+05,,,7.92E+02,,,,,,,,,,,,Statistical Alignment Model?,Academia,
,,,University of Montreal,Academia,"Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio",01/06/2007,2007,An empirical evaluation of deep architectures on problems with many factors of variation,https://dl.acm.org/doi/10.1145/1273496.1273556,1.12E+03,,Yes,,,,,,,,,,,,,,,,,Academia,
Restricted Bolzmann machines,Recommendation,,University of Toronto,Academia,"Russ Salukhutdinov, Andriy Mnih, GE Hinton",20/06/2007,2007,Restricted Boltzmann machines for collaborative filtering,https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8,2.14E+03,,Yes,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Academia,
,Recommendation,,Warsaw University,Academia,A Paterek,12/08/2007,2007,Improving regularized singular value decomposition for collaborative filtering,https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61,1.12E+03,Highly cited,Yes,,,Netflix Prize,1.00E+08,,,,,,,,,,,,,Academia,
,Vision,,"University of Bern, IDSIA, TU Munich",Academia,"M Liwicki, A Graves, S Fernàndez",23/09/2007,2007,A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks,https://people.idsia.ch//~juergen/icdar_2007.pdf,2.87E+02,,No,,,,,,,,,,,,,,,,,Academia,
,Recommendation,,AT&T Labs,Industry,"RM Bell, Y Koren",28/10/2007,2007,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,http://brettb.net/project/papers/2007%20Scalable%20collaborative%20filtering%20with%20jointly%20derived%20neighborhood%20interpolation%20weights.pdf,6.87E+02,,No,,,,,,,,,,,,,,,,,Industry,
,,,"NEC Laboratories, Google Zurich",Industry,"L Bottou, O Bousquet",03/12/2007,2007,The Tradeoffs of Large Scale Learning,https://dl.acm.org/doi/10.5555/2981562.2981583,1.70E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
BLSTM,Vision,Character recognition,"University of Bern, IDSIA, TU Munich",Academia,"Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández",03/12/2007,2007,Unconstrained online handwriting recognition with recurrent neural networks,https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html,3.41E+02,,No,1.01E+05,,,,,,,,,,,,,,,,Academia,
Stanley (DARPA Grand Challenge 2),Driving,Self-driving car,Stanford University,Industry - Academia Collaboration,"S Thrun, M Montemerlo, H Dahlkamp",01/01/2006,2006,Stanley: The Robot that Wonthe DARPA Grand Challenge,https://www.researchgate.net/publication/220648006_Stanley_The_robot_that_won_the_DARPA_Grand_Challenge,2.56E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,"CRAN, CENPARMI",Academia,"Fabian Lauer, Ching Y Suen, Gerard Bloch",02/02/2006,2006,A trainable feature extractor for handwritten digit recognition,https://hal.archives-ouvertes.fr/hal-00018426/en,3.65E+02,,No,,,,,,,,,,,,,,,,,Academia,
,Driving,Helicopter driving,Stanford and UC Berkeley,Academia,"H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng",09/03/2006,2006,Autonomous helicopter flight via reinforcement learning,https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,8.10E+01,,No,,,,,,,,,,,,,,,,,Academia,
FAST,Vision,Corner detection,University of Cambridge,Academia,Edward Rosten and Tom Drummond,07/05/2006,2006,Machine Learning for High-Speed Corner Detection,https://link.springer.com/chapter/10.1007/11744023_34,5.42E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,"University of Illinois, INRIA, Ecole Normale",Academia,"S Lazebnik, C Schmid, J Ponce",17/06/2006,2006,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf,9.81E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
DrLIM,Vision,Image embedding,New York University,Academia,R. Hadsell; S. Chopra; Y. LeCun,17/06/2006,2006,Dimensionality Reduction by Learning an Invariant Mapping,https://ieeexplore.ieee.org/document/1640964,2.69E+03,Highly cited,Yes,3.71E+04,,,2.17E+05,,,,,,,,,,,,,Academia,
CTC-Trained LSTM,Speech,Speech recognition,"IDSIA, TUM",Academia,"Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber",25/06/2006,2006,Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,https://www.cs.toronto.edu/~graves/icml_2006.pdf,3.29E+03,Highly cited,Yes,1.15E+05,,TIMIT,4.16E+04,,,,,,,,,,,,,Academia,
Semantic Taxonomy Induction,Language,,Stanford University,Academia,"Rion Snow, Dan Jurafsky, and Andrew Y. Ng",07/07/2006,2006,Semantic Taxonomy Induction from Heterogenous Evidence,https://www.aclweb.org/anthology/P06-1101/,5.71E+02,,No,1.00E+02,,,8.51E+05,,,,,,,,,,,,Mix of statistical and symbolic NLP,Academia,
DImensionality Reduction,Vision,Face recognition,University of Toronto,Academia,"GE Hinton, RR Salakhutdinov",18/07/2006,2006,Reducing the dimensionality of data with neural networks.,https://www.cs.toronto.edu/~hinton/science.pdf,1.57E+04,Highly cited,Yes,3.80E+06,,,7.00E+04,,,,,,,,,,,,,Academia,
Deep Belief Nets,Vision,Character recognition,"University of Toronto, NUS",Academia,"GE Hinton, S Osindero, YW Teh",18/07/2006,2006,A fast learning algorithm for deep belief nets,https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf,1.61E+04,Highly cited,Yes,1.60E+06,,MNIST,6.00E+04,,,,,,,,,,,,,Academia,
,,,"Machine Vision group, Finland",Academia,"Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",01/12/2006,2006,Face Description with Local Binary Patterns: Application to Face Recognition,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf,3.11E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,NYU,Academia,"M Ranzato, C Poultney, S Chopra, Y Cun",04/12/2006,2006,Efficient Learning of Sparse Representations with an Energy-Based Model.,https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html,1.60E+03,Highly cited,Yes,,,MNIST,6.00E+04,,,,,,,,,,,,,Academia,
,,,University of Montreal,Academia,"Y Bengio, P Lamblin, D Popovici",04/12/2006,2006,Greedy layer-wise training of deep networks,https://dl.acm.org/doi/10.5555/2976456.2976476,5.61E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
BiLSTM for Speech,Speech,Speech recognition,IDSIA and TU Munich,Academia,"A Graves, J Schmidhuber",01/08/2005,2005,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206,3.39E+03,Highly cited,Yes,1.52E+05,2.41E+13,TIMIT,4.16E+04,,,,,,,,,,,,,Academia,
SACHS,Other,,MIT and Stanford,Academia,"K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan",22/04/2005,2005,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.,https://science.sciencemag.org/content/308/5721/523.long,1.68E+03,Highly cited,Yes,1.78E+02,,,5.40E+03,,,,,,,,,,,,,Academia,
Hiero,Language,Translation,"University of Maryland, College Park",Academia,David Chiang,01/06/2005,2005,A Hierarchical Phrase-Based Model for Statistical Machine Translation,https://aclanthology.org/P05-1033/,1.49E+03,Highly cited,Yes,1.20E+08,,,1.71E+08,,,,,,,,,,,,Statistical Phrase-Based Model,Academia,
,,,University of the Balearic Islands and CMLA,Academia,"A Buades, B Coll, JM Morel",20/06/2005,2005,A non-local algorithm for image denoising,http://www.iro.umontreal.ca/~mignotte/IFT6150/Articles/Buades-NonLocal.pdf,6.87E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,New York University,Academia,"S Chopra, R Hadsell, Y LeCun",20/06/2005,2005,"Learning a similarity metric discriminatively, with application to face verification",https://ieeexplore.ieee.org/document/1467314,3.05E+03,Highly cited,Yes,,,,1.40E+05,,,,,,,,,,,,,Academia,
,Vision,,Inria Grenoble Rhône-Alpes,Academia,"N Dalal, B Triggs",25/06/2005,2005,Histograms of oriented gradients for human detection,https://ieeexplore.ieee.org/document/1467360,3.66E+04,Highly cited,Yes,,,,1.81E+03,,,,,,,,,,,,,Academia,
,,,Stanford University,Academia,"B. Taskar, C. Guestrin, and D. Koller",01/03/2004,2004,Max-margin markov networks,https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf,1.76E+03,Highly cited,Yes,,,,6.00E+02,,,,,,,,,,,,,Academia,
,,,Soongsil University,Academia,"KS Oh, K Jung",01/06/2004,2004,GPU implementation of neural networks,https://www.sciencedirect.com/science/article/pii/S0031320304000524,4.71E+02,,No,,,,,,,,,,,,,,,,,Academia,
Sandstorm (DARPA Grand Challenge I),Driving,Self-driving car,Carnegie Mellon University,Academia,William Red L. Whittaker,14/06/2004,2004,DARPA Grand Challenge Technical Paper,https://ieeexplore.ieee.org/document/1336386,6.60E+01,,No,,,,,,,,,,,,,,,,,Academia,
,Language,Word sense disambiguation,University of Sussex,Academia,"D McCarthy, R Koeling, J Weeds",01/07/2004,2004,Finding Predominant Word Senses in Untagged Text,https://aclanthology.org/P04-1036/,4.75E+02,,No,,,,5.00E+03,,,,,,,,,,,,,Academia,
LIRA,Vision,Character recognition,Instituto de Ciencias Aplicadas y Technologia,Academia,"E Kussul, T Baidyk",30/07/2004,2004,Improved method of handwritten digit recognition tested on MNIST database,https://www.sciencedirect.com/science/article/abs/pii/S0262885604000721,1.88E+02,,No,1.00E+05,,,1.00E+04,1,,,45,500,,,,,,,,Academia,
NPLM,Language,Text autocompletion,Université de Montréal,Academia,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",15/03/2003,2003,A Neural Probabilistic Language Model,https://dl.acm.org/doi/10.5555/944919.944966,7.63E+03,Highly cited,Yes,1.19E+07,1.30E+15,Brown corpus,1.00E+06,,2.17E+07,,20160,,,,,,,,,Academia,
,Recommendation,,University of Washington,Industry - Academia Collaboration,"G. Linden, B. Smith, and J. York",01/01/2003,2003,Amazon.com Recommendations: Item-to-Item Collaborative Filtering,https://ieeexplore.ieee.org/document/1167344,7.26E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Vision,Image Classification,California Institute of Technology,Academia,"M. Weber, M. Welling, and P. Perona",01/01/2003,2003,Unsupervised Learning of Models for Recognition,https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf,9.49E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
LDA,Language,Document classification,"University of California, Stanford University",Academia,"David M. Blei, Andrew Y. Ng, Michael I. Jordan",02/02/2003,2003,Latent Dirichlet Allocation,https://jmlr.org/papers/volume3/blei03a/blei03a.pdf,3.87E+04,Highly cited,Yes,,,,,,,,,,,,,,,,Hierarchial Bayesian Model/Generative Probabilistic Model,Academia,
Phrase-based translation,Language,Translation,University of Southern California,Academia,"Philipp Koehn, Franz Josef Och, Daniel Marcu",01/05/2003,2003,Statistical Phrase-Based Translation,https://dl.acm.org/doi/10.3115/1073445.1073462,4.27E+03,Highly cited,Yes,9.18E+06,,,2.00E+07,,,,,,,,,,,,,Academia,
Unsupervised Scale-Invariant Learning,Vision,,University of Oxford,Academia,"R Fergus, P Perona, A Zisserman",18/06/2003,2003,Object Class Recognition by Unsupervised Scale-Invariant Learning,https://ieeexplore.ieee.org/document/1211479,2.97E+03,Highly cited,Yes,4.51E+02,,,3.50E+03,,,,,,,,,,,,,Academia,
CNN Best Practices,Vision,Character recognition,One Microsoft Way,Industry,"PY Simard, D Steinkraus, JC Platt",06/08/2003,2003,Best practices for convolutional neural networks applied to visual document analysis,https://ieeexplore.ieee.org/document/1227801,3.07E+03,Highly cited,Yes,,,MNIST,5.00E+04,,,,,,,,,,,,,Industry,
Thumbs Up?,Language,Sentiment classification,Cornell University and IBM Almaden Research Center,Industry - Academia Collaboration,"Bo Pang, Lillian Lee, Shivakumar Vaithyanathan",28/05/2002,2002,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://arxiv.org/abs/cs/0205070,1.07E+04,Highly cited,Yes,,,IMDb,2.05E+03,,,,,,,,,,,,,Industry,
,Language,,AT&T Labs,Industry,Michael Collins,01/06/2002,2002,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,https://dl.acm.org/doi/10.3115/1118693.1118694,2.58E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,Language,,University of Southern California,Industry - Academia Collaboration,Daniel Marcu and William Wong,01/06/2002,2002,"A Phrase-Based, Joint Probability Model for Statistical Machine Translation",https://dl.acm.org/doi/10.3115/1118693.1118711,6.23E+02,,No,,,Hansard Corpus,1.07E+06,,,,,,,,,,,,Statistical Alignment Model,Industry,
,,,IDSIA Switzerland,Academia,"Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber",01/06/2002,2002,Evolving Neural Networks through Augmenting Topologies ,https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting,3.37E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Language,Translation,RWTH Aachen and University of Southern California,Academia,Franz Josef Och and Hermann Ney,06/07/2002,2002,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,https://aclanthology.org/P02-1038/,1.41E+03,Highly cited,Yes,,,,5.20E+05,,,,,,,,,,,,,Academia,
,Language,,IBM TJ Watson Research Centre,Industry,"K Papineni, S Roukos, T Ward, WJ Zhu",06/07/2002,2002,Bleu: a method for automatic evaluation of machine translation,https://dl.acm.org/doi/10.3115/1073083.1073135,1.58E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,Korea Advanced Institute of Science and Technology,Academia,"YH Cho, JK Kim, SH Kim",01/10/2002,2002,A personalized recommender system based on web usage mining and decision tree induction,https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441,6.56E+02,,No,,,,,,,,,,,,,,,,,Academia,
Decision tree (classification),Vision,Face recognition,Mitsubishi Electric Research Labs and Compaq CRL,Industry - Academia Collaboration,"P. Viola, M. Jones",08/12/2001,2001,Rapid object detection using a boosted cascade of simple features,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,2.34E+04,Highly cited,Yes,1.20E+08,6.30E+13,,1.45E+04,,6.70E+07,,,,,,,,,,,Industry,
Immediate trihead,Language,,Brown University,Academia,E Charniak,06/07/2001,2001,Immediate-Head Parsing for Language Models,https://dl.acm.org/doi/10.3115/1073012.1073029,4.22E+02,,No,,,,,,,,,,,,,,,,,Academia,
,,,Stanford University,Academia,Jerome H. Friedman,01/10/2001,2001,Greedy function approximation: A gradient boosting machine,https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full,1.43E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Recommendation,,University of Minnesota,Academia,"B Sarwar, G Karypis, J Konstan, J Riedl",14/07/2000,2000,Application of Dimensionality Reduction in Recommender System -- A Case Study,http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf,2.13E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Peephole LSTM,Other,Periodic function approximation,IDSIA Switzerland,Academia,F.A. Gers; J. Schmidhuber,26/07/2000,2000,Recurrent nets that time and count,https://ieeexplore.ieee.org/document/861302,6.30E+02,,No,1.70E+01,,,6.50E+07,,,,,,,,,,,,,Academia,
,Language,,University of Rochester,Academia,"Daniel Gildea, Daniel Jurafsky",01/09/2000,2000,Automatic Labeling of Semantic Roles,https://dl.acm.org/doi/10.1162/089120102760275983,2.33E+03,Highly cited,Yes,,,FrameNet,5.00E+04,,,,,,,,,,,,,Academia,
,Language,,RWTH Aachen - University of Technology,Academia,"Franz Josef Och, Hermann Ney",03/10/2000,2000,Improved Statistical Alignment Models,https://aclanthology.org/P00-1056/,1.32E+03,Highly cited,Yes,,,,,,,,,,,,,,,,Statistical Alignment Model,Academia,
LSTM with forget gates,,,IDSIA Switzerland,Academia,"F. A. Gers, J. Schmidhuber, and F. Cummins",02/01/1999,1999,Learning to forget: Continual prediction with LSTM,https://ieeexplore.ieee.org/document/818041,4.52E+03,Highly cited,Yes,2.76E+02,,,3.00E+04,,,,,,,,,,,,,Academia,
IBM Model 4,Language,Translation,University of Southern California & IBM & University of Pennsylvania,Industry - Academia Collaboration (Academia leaning),"Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky",02/07/1999,1999,Statistical machine translation,http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf,1.92E+03,Highly cited,Yes,,,,8.00E+05,,,,,,,,,,,,,Industry,
,Vision,Character recognition,"University of California San Diego & Shannon Laboratory, AT&T Labs",Industry,Yoav Freund & Robert E. Schapire,01/12/1999,1999,Large Margin Classification Using the Perceptron Algorithm,https://link.springer.com/article/10.1023/A:1007662407062,1.73E+03,Highly cited,Yes,,,MNIST,6.00E+04,,,,,,,,,,,,,Industry,
LeNet-5,Vision,Character recognition,AT&T Labs,Industry - Academia Collaboration (Industry leaning),"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner",01/11/1998,1998,Gradient-based Learning Applied to Document Recognition,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,3.86E+04,Historical relevance,Yes,6.00E+04,2.81E+12,MNIST,6.00E+04,6,7.81E+05,,,,0.021,,,,,,,Industry,
RNN for speech,Speech,Speech synthesis,National Chiao Tung University,Academia,"SH Chen, SH Hwang, YR Wang",15/05/1998,1998,An RNN-based prosodic information synthesizer for Mandarin text-to-speech,https://ieeexplore.ieee.org/abstract/document/668817,2.31E+02,,No,7.51E+03,2.27E+11,,1.41E+04,,,,,,,,,,,,,Academia,
,Speech,,Johns Hopkins University,Academia,F Jelinek,15/01/1998,1998,Statistical Methods for Speech Recognition,https://mitpress.mit.edu/books/statistical-methods-speech-recognition,3.06E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,Face recognition,Carnegie Mellon University,Academia,"H Schneiderman, T Kanade",23/06/1998,1998,Probabilistic modeling of local appearance and spatial relationships for object recognition,https://ieeexplore.ieee.org/document/698586,6.02E+02,,No,,,,1.20E+05,,,,,,,,,,,,,Academia,
,Other,Recommender system,AT&T Labs and Rutgers University and Bell Communications Research,Industry - Academia Collaboration,"C Basu, H Hirsh, W Cohen",01/07/1998,1998,Recommendation as Classification : Using Social and Content-based Information in Recommendation,https://www.aaai.org/Papers/AAAI/1998/AAAI98-101.pdf,1.56E+03,Highly cited,Yes,,,,4.50E+04,,,,,,,,,,,,,Industry,
LSTM,Other,Sequence recognition (?),The Technical University of Munich,Academia,Sepp Hochreiter ; Jurgen Schmidhuber,15/11/1997,1997,Long short-term memory,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,5.20E+04,Highly cited,Yes,1.05E+04,2.10E+13,,1.27E+06,,4.20E+04,,,,,,,,,,,Academia,
,Vision,,MIT,Academia,"E. Osuna, R. Freund, F. Girosi",17/06/1997,1997,Training Support Vector Machines: An Application to Face Detection,https://ieeexplore.ieee.org/document/609310,3.85E+03,Highly cited,Yes,,,,5.00E+04,,,,,,,,,,,,,Academia,
,Language,,Cambridge University Engineering & Carnegie Mellon University,Academia,"P Clarkson, R Rosenfeld",01/07/1997,1997,Statistical language modeling using the CMU-Cambridge toolkit,https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87,9.54E+02,,No,,,,,,,,,,,Supervised,,,,,,Academia,
BRNN,Speech,Speech recognition,"ATR Labs, Japan",Industry,"M. Schuster, KK Paliwal",01/11/1997,1997,Bidirectional recurrent neural networks,https://ieeexplore.ieee.org/document/650093,6.09E+03,Highly cited,Yes,1.30E+04,,TIMIT,7.39E+04,,,,,,,,,,,,,Industry,
,,,"UC Davis, Cornell",Academia,"Bruno A. Olshausen, David J. Field",01/12/1997,1997,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.sciencedirect.com/science/article/pii/S0042698997001697,4.26E+03,Highly cited,Yes,,,,1.00E+01,,,,,,,,,,,,,Academia,
System 11,Vision,Face recognition,Carnegie Mellon University,Academia,"HA Rowley, S Baluja, T Kanade",18/06/1996,1996,Neural Network-Based Face Detection,https://ieeexplore.ieee.org/document/655647,6.01E+03,Highly cited,Yes,6.45E+03,9.58E+08,,9.05E+03,,1.29E+04,,,,,,,,,,,Academia,
HMM Word Alignment,Language,Word alignment,University of Erlangen - Nuremburg,Academia,"Stephan Vogel, Hermann Ney, Christoph Tillmann",05/08/1996,1996,HMM-Based Word Alignment in Statistical Translation,https://dl.acm.org/doi/10.3115/993268.993313,1.10E+03,Highly cited,Yes,,,,4.42E+05,,,,,,,Supervised,,,,,Statistical Alignment Model,Academia,
,,,Xerox,Industry,Eric Saund,01/01/1995,1995,A Multiple Cause Mixture Model for Unsupervised Learning,https://ieeexplore.ieee.org/document/6795568,1.76E+02,,No,,,,,,,,,,,,,Perplexity... sorta,,,,Industry,
,Language,,University of Pennsylvania,Academia,D Yarowsky,26/06/1995,1995,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://dl.acm.org/doi/10.3115/981658.981684,3.00E+03,Highly cited,Yes,,,,4.60E+08,,,,,,,,,,,,Bootstrapping classifier,Academia,
Random Decision Forests,,,AT&T Bell Laboratories,Industry,TK Ho,14/08/1995,1995,Random decision forests,https://ieeexplore.ieee.org/document/598994,4.68E+03,Highly cited,Yes,,,MNIST,6.00E+04,,,,,,,,,,,,,Industry,
Support Vector Machines,,,AT&T Bell Laboratories,Industry,"C Cortes, V Vapnik",01/09/1995,1995,Support-Vector Networks,https://link.springer.com/article/10.1007/BF00994018,4.90E+04,Highly cited,Yes,1.00E+08,,MNIST,6.00E+04,,,,,,,,,,,,,Industry,
,Language,Part-of-speech tagging,EURECOM,Academia,Bernard Merialdo,01/06/1994,1994,Tagging English Text with a Probabilistic Model,https://dl.acm.org/doi/10.5555/972525.972526,7.88E+02,,No,2.45E+06,,,1.00E+06,N/A,,,,,,,,,,,,Academia,
GroupLens,Recommendation,,Massachusetts Institute of Technology,Academia,"Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl",22/10/1994,1994,GroupLens : an Open Architecture for Collaborative Filtering of Netnews,https://dl.acm.org/doi/10.1145/192844.192905,7.73E+03,Highly cited,Yes,1.00E+08,,,1.00E+08,,,,,,,,,,,,,Academia,
IBM-5,Language,Translation,IBM T.J. Watson Research Center,Industry,"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer",15/06/1993,1993,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://dl.acm.org/doi/10.5555/972470.972474,5.75E+03,Highly cited,Yes,1.66E+06,,Proceedings of the Canadian parliament,5.34E+07,,,,,,,,,,,,Statistical Alignment Model,Industry,
TD-Gammon,Games,Backgammon,IBM,Industry,G Tesauro,01/05/1992,1992,Practical Issues in Temporal Difference Learning,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1.34E+03,Highly cited,Yes,2.50E+04,1.82E+13,,6.30E+06,1,,,,,,,,,,,,Industry,
Fuzzy NN,Speech,Speech recognition,Indian Statistical Institute,Academia,"SK Pal, S Mitra",01/09/1992,1992,"Multilayer perceptron, fuzzy sets, and classification",https://ieeexplore.ieee.org/document/159058,1.22E+03,Highly cited,Yes,1.17E+03,3.05E+06,,8.71E+02,,,,,,,,,,,,,Academia,
,,,Northeastern University,Academia,R. J. Williams,01/05/1992,1992,Simple statistical gradient-following algorithms for connectionist reinforcement learning,https://dl.acm.org/doi/10.1007/BF00992696,6.53E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
DIABETES,Other,Medical diagnosis,Aalborg University,Academia,"S. Andreassen, R. Hovorka, J. Benn, K. G. Olesen, and E. R. Carson",24/06/1991,1991,A Model-based Approach to Insulin Adjustment,https://link.springer.com/chapter/10.1007/978-3-642-48650-0_19,1.32E+02,,No,4.29E+05,,,,,,,,,,,,,,,,Academia,
,Language,,"University of California, San Diego",Academia,J. L. Elman,01/09/1991,1991,"Distributed representations, simple recurrent networks, and grammatical structure",https://dl.acm.org/doi/10.1007/BF00114844,1.72E+03,Highly cited,Yes,,,,1.78E+05,,,,,,,,,,,,"Recurrent Network (""Elman"" network?)",Academia,
MADALINE III,,,University of Stanford,Academia,"B Widrow, M. A. Lehr",01/09/1990,1990,"30 years of adaptive neural networks: perceptron, madaline, and backpropagation",https://ieeexplore.ieee.org/document/58323,3.01E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,"Air Force Institute of Technology, OH, USA",Academia,D.W. Ruck & S.K. Rogers & M. Kabrisky & M.E. Oxley & B.W. Suter,01/12/1990,1990,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,https://ieeexplore.ieee.org/abstract/document/80266,1.05E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
ALVINN,Driving,,Carnegie Mellon University ,Academia,DA Pomerleau,01/12/1989,1989,ALVINN: an autonomous land vehicle in a neural network,https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,1.58E+03,Highly cited,Yes,3.99E+03,8.12E+10,Road snapshots,1.20E+03,1,,,,,,,,,,,,Academia,
Zip CNN,Vision,Character recognition,AT&T Bell Laboratories,Industry,Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,01/12/1989,1989,Backpropagation applied to handwritten zip code recognition,https://ieeexplore.ieee.org/document/6795724,9.05E+03,Highly cited,Yes,9.76E+03,4.34E+10,Buffalo zips,7.29E+03,3,1.29E+05,,,,,,,,,,,Industry,
Innervator,Other,Pattern classification,"Stanford, CalTech",Academia,"Geoffrey Miller, Peter Todd, and Shailesh Hegde",01/01/1989,1989,Designing neural networks using genetic algorithms,https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms,1.13E+03,Highly cited,Yes,1.00E+01,1.20E+08,,4.00E+00,,,,,,,,,,,,,,
Q-learning,,,University of London,Academia,Christopher Watkins,01/01/1989,1989,Learning from delayed rewards,http://www.cs.rhul.ac.uk/~chrisw/thesis.html,8.03E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Time-delay neural networks,,,Carnegie Mellon University & ATR Interpreting Telephony Research Laboratories & University of Toronto ,Industry - Academia Collaboration,"A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang",03/03/1989,1989,Phoneme recognition using time-delay neural networks,https://ieeexplore.ieee.org/abstract/document/21701,3.45E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
,,,Technische Universität Wien Austria & University of California,Academia,Kurt Hornik & Maxwell Stinchcombe & Halbert White,09/03/1989,1989,Multilayer feedforward networks are universal approximators,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208,2.17E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,Roke Manor Research,Industry,Harris & Stephens,01/07/1988,1988,A Combined Corner and Edge Detector,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf,1.91E+04,Highly cited,Yes,,,,1.50E+03,,,,,,,,,,,,,Industry,
MADALINE II,Other,Pattern classification,Stanford University,Academia,"Rodney Winter, Bernard Widrow",24/07/1988,1988,MADALINE RULE II: A Training Algorithm for Neural Networks,https://ieeexplore.ieee.org/document/23872,8.10E+01,,No,,,,,,,,,,,,,,,,,Academia,
Adaptive Broom Balancer,Games,Pole balancing,Stanford University,Academia,"VV Tolat, B Widrow",24/07/1988,1988,An Adaptive “Broom Balancer” with Visual Inputs,https://ieeexplore.ieee.org/document/23982,8.00E+01,,No,1.10E+02,,,,,,,,,,,,,,,,Academia,
NetTalk,Speech,Speech synthesis,Princeton University,Academia,"TJ Sejnowski, CR Rosenberg",06/06/1987,1987,Parallel Networks that Learn to Pronounce English Text,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2.56E+03,Highly cited,Yes,1.86E+04,8.12E+10,,2.10E+04,1,,,,,,,,,,,,Academia,
,Vision,,"University of California, Santa Cruz",Academia,"Biederman, Irving",01/04/1987,1987,Recognition-by-components: A theory of human image understanding,https://psycnet.apa.org/record/1987-20898-001,7.59E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Absity,Language,Semantic Interpretation,Brown University,Academia,Hirst,01/01/1987,1987,The Absity semantic interpreter,https://ftp.cs.toronto.edu/pub/gh/Hirst%201987%20book%20by%20chapter/06.2_pp_44_74_The_Absity_semantic_interpreter.pdf,9.80E+01,,No,,,,,,,,,,,,,,,,,,
Back-propagation,Other,Learning to complete triples,University of California,Academia,"Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",01/10/1986,1986,Learning representations by back-propagating errors,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,2.53E+04,Highly cited,Yes,1.44E+02,1.24E+08,,1.44E+02,,2.88E+02,,,,,Unsupervised,,,,,,Academia,
,,,University of California and University of Carnegie Mellon,Academia,"D. E. Rumelhart, G. E. Hinton, and R. J. Williams",03/01/1986,1986,Learning internal representations by error propagation,https://dl.acm.org/doi/10.5555/104279.104293,2.73E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Learning past tenses,Language,Verb conjugation,Stanford,Academia,"Rumelhart, D. E., & McClelland, J. L",03/01/1986,1986,Learning the past tenses of English verbs: Implicit rules or parallel distributed processing?,https://www.semanticscholar.org/paper/On-learning-the-past-tenses-of-English-verbs%3A-rules-Rumelhart-McClelland/4fa569625b5ab35e955a8d5be11a4aa9f59ca424,3.18E+02,,No,2.12E+05,,,,,,,,,,,,,,,Parallel Distributed Processing Model,Industry,
,,,University of California,Academia,"Jordan, M.I.",05/01/1986,1986,Serial order: A parallel distributed processing approach,https://www.osti.gov/biblio/6910294,1.50E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,Vision,,Massachusetts Institute of Technology,Academia,John Canny,01/11/1986,1986,A Computational Approach To Edge Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851,3.79E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
MOPTRANS,Language,Syntax Representation,"Cognitive Systems, Inc.",Industry,Lytinen,14/08/1985,1985,INTEGRATING SYNTAX AND SEMANTICS,https://aclanthology.org/www.mt-archive.info/70/TMI-1985-Lytinen.pdf,3.90E+01,,No,,,,,,,,,,,,,,,,,,
,Language,,MIT,Academia,Steven Pinker,01/07/1984,1984,Language learnability and language development.,https://psycnet.apa.org/record/1985-97439-000,4.73E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Racter,Language,Text Generation,William Chamberlain and Thomas Etter,Industry,William Chamberlain and Thomas Etter,01/10/1984,1984,The Policeman's Beard Is Half Constructed,https://www.ubu.com/media/text/racter/racter_policemansbeard.pdf,,Historical relevance,Yes,,1.40E+06,,,,,,,,,,,,,,,,
ASE+ACE,Games,Pole balancing,Stanford,Academia,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson",01/09/1983,1983,Neuronlike adaptive elements that can solve difficult learning control problems,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,4.30E+03,Highly cited,Yes,3.24E+02,,,,,,,,,,,,,,,,Academia,
Hopfield network,Other,Sequence memorization,California Institute of Technology,Academia,JJ Hopfield,01/04/1982,1982,Neural networks and physical systems with emergent collective computational abilities,https://www.pnas.org/doi/10.1073/pnas.79.8.2554,2.33E+04,Highly cited,Yes,9.90E+03,,,,N/A,,,,,,,,,,,,Academia,
Kohonen network,Other,Dimensionality reduction,Helsinki University of Technology,Academia,T Kohonen,25/07/1981,1981,Self-organized formation of topologically correct feature maps,https://link.springer.com/article/10.1007/BF00337288,1.18E+04,Highly cited,Yes,4.10E+03,,,,,,,,,,,,,,,,Academia,
Plot Units,Language,Text Summarization,Yale,Academia,Lehnert,01/12/1981,1981,Plot Units and Narrative Summarization,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1207/s15516709cog0504_1,5.79E+02,,No,,,,,,,,,,,,,,,,,,
Neocognitron,Vision,Character recognition,NHK Broadcasting Science Research Laboratories,Industry,"K Fukushima, S Miyake",01/04/1980,1980,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://link.springer.com/article/10.1007/BF00344251,5.78E+03,Highly cited,Yes,1.14E+06,2.28E+08,,5.00E+00,,,,,,,,,,,,,Industry,
,Vision,,Utrecht University,Academia,Koenderink & van Doom,02/05/1979,1979,The internal representation of solid shape with respect to vision,https://link.springer.com/article/10.1007/BF00337644,9.81E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
Politics,Language,Natural Language Inference,Yale,Academia,Carbonell,01/01/1978,1978,POLITICS: Automated Ideological Reasoning,https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0201_3,2.13E+02,,No,,,,,,,,,,,,,,,,,,
TD(0),,,University of Essex,Academia,Ian Witten,01/08/1977,1977,An adaptive optimal controller for discrete-time Markov environments,https://www.sciencedirect.com/science/article/pii/S0019995877903540,2.69E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
TaleSpin,Language,Text Generation,UCI,Academia,Meehan,22/08/1977,1977,"TALE-SPIN, An Interactive Program that Writes Stories",https://www.ijcai.org/Proceedings/77-1/Papers/013.pdf,5.45E+02,,No,,1.00E+06,,,,,,,,,,,,,,,,
QUALM,Language,Question Answering,Yale,Academia,Lehnert,22/08/1977,1977,A conceptual theory of question answering,https://dl.acm.org/doi/10.5555/1624435.1624467,1.32E+02,,No,,,,,,,,,,,,,,,,,,
LIFER/LADDER,Language,Question Answering,Stanford,Academia,Hendrix,01/02/1977,1977,LIFER: A NATURAL LANGUAGE INTERFACE FACILITY,https://dl.acm.org/doi/pdf/10.1145/1045283.1045289,5.10E+01,,No,,1.00E+06,,,,,,,,,,,,,,,,
SAM,Language,Question Answering,Yale,Academia,Cullingford,01/02/1977,1977,SAM: A Program That Uses World Knowledge To Understand ,https://dl.acm.org/doi/pdf/10.1145/1045283.1045324,1.00E+00,Historical relevance,Yes,,,,,,,,,,,,,,,,,,
PAM,Language,Natural Language Inference,Yale,Academia,Robert Wilensky,22/08/1977,1977,PAM - A Program That Infers Intentions,https://www.ijcai.org/Proceedings/77-1/Papers/003A.pdf,2.10E+01,,No,,,,,,,,,,,,,,,,,,
Cognitron,,,Biological Cybernetics,Industry,Kunihiko Fukushima,01/09/1975,1975,Cognitron: a self-organizing multilayered neural network,https://link.springer.com/article/10.1007%2FBF00342633,7.91E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Industry,
Naive Bayes,Vision,,Stanford Research Institute,Industry,Duda and Hart,01/09/1974,1974,Pattern Classification and Scene Analysis,https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188,2.31E+04,Highly cited,Yes,,,,,,,,,,,,,,,,,Industry,
Bootstrap Adaptation,Games,Blackjack,IEEE,Academia,"Widrow, Gupta, and Maitra",01/09/1973,1973,Punish/Reward: Learning with a Critic in Adaptive Threshold Systems,https://ieeexplore.ieee.org/document/4309272,3.97E+02,,No,2.10E+01,,,,,,,,,,,,,,,,Academia,
MARGIE,Language,Natural Language Inference,Stanford,Academia,Roger Schank,20/08/1973,1973,"MARGIE MEMORY, ANALYSIS, RESPONSE GENERATION, and INFERENCE on ENGLISH",https://www.ijcai.org/Proceedings/73/Papers/028.pdf,1.01E+02,,No,,,,,,,,,,,,,,,,,,
PARRY,Language,Question Answering,Stanford,Academia,Kenneth Colby,01/01/1972,1972,Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes,https://www.sciencedirect.com/science/article/abs/pii/0004370272900495,1.94E+02,,No,,2.50E+05,,,,,,,,,,,,,,,,
Punish/Reward,,,Massachusetts Institute of Technology,Academia,Patrick Winston,01/09/1970,1970,Learning Structural Definitions from Examples,https://dspace.mit.edu/handle/1721.1/6884,1.81E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
SHRDLU,Language,Question Answering,MIT,Academia,Terry Winograd,24/08/1970,1970,Procedures as a Representation for Data in a Computer Program for Understanding Natural Language,http://hci.stanford.edu/winograd/shrdlu/AITR-235.pdf,8.22E+02,,No,,2.50E+05,,,,,,,,,,,,,,,,
GLEE,Games,Tic Tac Toe,University of Edinburgh,Academia,Michie and Chambers,01/07/1968,1968,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,5.90E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
BOXES,Games,Pole balancing,University of Edinburgh,Academia,Michie and Chambers,01/07/1968,1968,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,5.90E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
Samuel Neural Checkers II,Games,Checkers,University of Geneva,Academia,"Palmieri, G. and R. Sanna",01/11/1967,1967,Some studies in machine learning using the game of checkers. Part II,https://www.cs.virginia.edu/~evans/greatworks/samuel.pdf,7.47E+02,,No,4.00E+01,,,,N/A,,,,,,,,,,,,Academia,
ELIZA,Language,Question Answering,MIT,Academia,Joseph Weizenbaum,01/01/1966,1966,ELIZA—a computer program for the study of natural language communication between man and machine,https://dl.acm.org/doi/10.1145/365153.365168,6.31E+03,Highly cited,Yes,,1.00E+05,,,,,,,,,,,,,,,,
STUDENT,Language,Question Answering ,MIT,Academia,Daniel Bobrow,03/01/1964,1964,Natural Language Input for a Computer Problem Solving System,https://dspace.mit.edu/handle/1721.1/5922,6.46E+02,,Yes,,,,,,,,,,,,,,,,,,
STeLLA,,,University of Canterbury,Academia,J.H. Andreae and Peter L. Joyce,01/06/1963,1963,STeLLA: A Scheme for a Learning Machine,https://www.researchgate.net/publication/252919025_STELLA_A_scheme_for_a_learning_machine,3.40E+01,,No,,,,,,,,,,,,,,,,,Academia,
MENACE,Games,Tic Tac Toe,University of Edinburgh,Academia,Donald Michie,01/11/1963,1963,Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters,https://academic.oup.com/comjnl/article/6/3/232/360077,4.60E+01,,No,,,,,,,,,,,,,,,,,Academia,
MADALINE I,,,Stanford University,Academia,William Combs Ridgway,01/07/1962,1962,An adaptive logic system with generalizing properties,https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?,7.50E+01,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Massachusetts Institute of Technology (MIT),Academia,Marvin Minsky,01/01/1961,1961,Steps Toward Artificial Intelligence,https://ieeexplore.ieee.org/abstract/document/4066245,2.43E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Massachusetts Institute of Technology (MIT),Academia,Marvin Minsky and Oliver G. Selfridge,01/07/1961,1961,Learning in random nets,https://stacks.stanford.edu/file/druid:yr384hg3073/yr384hg3073.pdf,4.70E+01,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
,,Binary classification,The University of Genoa,Academia,"A Gamba, L Gamberini, G Palmieri, R Sanna",01/09/1961,1961,Further experiments with PAPA,https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2,2.40E+01,,No,,,,,,,,,,,,,,,,,Academia,
ADALINE,Vision,Pattern recognition,Stanford University,Academia,Widrow and Hoff,30/06/1960,1960,Adaptive switching circuits,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,6.33E+03,Highly cited,Yes,1.70E+01,9.90E+03,,1.00E+02,,3.30E+01,,,,,,,,,,,Academia,
LMS,,,Stanford University,Academia,Widrow and Hoff,30/06/1960,1960,Adaptive switching circuits (technical report),https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230,6.33E+03,Highly cited,Yes,,,,,,,,,,,,,,,,,Academia,
Pandemonium (morse),Other,Morse translation,Massachusetts Institute of Technology,Academia,OG Selfridge,01/02/1959,1959,Pandemonium: A Paradigm for Learning,https://aitopics.org/doc/classics:504E1BAC/,1.45E+03,Highly cited,Yes,3.00E+03,6.00E+08,,,,,,,,,,,,,,,Academia,
Samuel Neural Checkers,Games,Checkers,IBM,Industry,Arthur L. Samuel,01/07/1959,1959,Some studies in machine learning using the game of checkers,https://ieeexplore.ieee.org/abstract/document/5392560,4.15E+03,Highly cited,Yes,1.60E+01,4.28E+08,,5.30E+04,N/A,,,9,7.5,,,,,,,,Industry,
Pattern recognition and reading by machine,Vision,Character recognition,Sandia Corporation,Industry,"W. W. Bledsoe, I. Browning",01/12/1959,1959,Pattern recognition and reading by machine,https://dl.acm.org/doi/10.1145/1460299.1460326,5.87E+02,Historical relevance,Yes,2.63E+03,,,,,,,,,,,,,,,,Industry,
Perceptron Mark I,Vision,Binary classification,Cornell Aeronautical Laboratory,Industry,F Rosenblatt,01/01/1957,1957,The Perceptron—a perceiving and recognizing automaton,https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf,1.61E+03,Historical relevance,Yes,4.00E+02,6.95E+05,,6.00E+00,N/A,,,,,,,,N/A,,,,Industry,
,Vision,,Princeton University,Academia,AM Uttley,01/07/1956,1956,Conditional probability machines,https://www.moma.org/collection/works/illustratedbooks/16252?locale=es,8.40E+01,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
Self Organizing System,Vision,Pattern recognition,Massachusetts Institute of Technology,Academia,W. A. Clark and B. G. Farley,01/03/1955,1955,Generalization of pattern recognition in a self-organizing system,https://dl.acm.org/doi/10.1145/1455292.1455309,9.30E+01,Historical relevance,Yes,2.25E+02,,,2.56E+02,,,,,,,,,,,,,Academia,
,Vision,Character recognition,Massachusetts Institute of Technology,Academia,O. G. Selfridge,01/03/1955,1955,Pattern recognition and learning,https://dl.acm.org/doi/10.1145/1455292.1455310,2.90E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
,,,Institute for Advanced Study,Academia,NA Barricelli,02/07/1954,1954,Esempi numerici di processi di evoluzione,https://link.springer.com/article/10.1007/BF01556771,2.66E+02,Historical relevance,Yes,,,,,,,,,,,,,,,,,Academia,
Georgetown experiment,Language,Machine Translation (RU-EN),Georgetown University and IBM,Industry - Academia Collaboration,Georgetown University and IBM,07/01/1954,1954,"The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954",https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment,1.06E+02,Historical relevance,Yes,,2.00E+03,,,,,,,,,,,,,,,,
SNARC,Other,Maze solving,Harvard University Psychological Laboratories,Academia,Marvin Minsky,08/01/1952,1952,A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement,https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator,3.30E+01,Historical relevance,Yes,4.00E+01,,,,,,,,,,,,,,,,Academia,
Theseus,Other,Maze solving,Bell Laboratories,Industry,Claude Shannon,02/07/1950,1950,Mighty Mouse,https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/,N/A,Historical relevance,Yes,4.00E+01,4.00E+01,,4.00E+01,,,,,,,,,,,,,Industry,